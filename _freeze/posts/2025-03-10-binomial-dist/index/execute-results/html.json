{
  "hash": "9d5408cdc9ce6911509744507d184653",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understanding Martin Bland's 'Deceting a single event'\"\ndescription: \"Exploring the math behind the method.\"\nauthor:\n  - name: Jess Graves\ndate: 03-10-2025\nexecute-dir: project\ncrossref:\n  fig-title: '**Figure**'\n  tbl-title: '**Table**'\n  fig-labels: arabic\n  tbl-labels: arabic\n  title-delim: \".\"\nlink-citations: true\nexecute:\n  echo: true\n  warning: false\n  message: false\ncategories: [tidy tuesday, data visualization] # self-defined categories\nimage: preview-image.png\ndraft: true  \n# bibliography: references.bib\nnocite: |\n  @*\n# csl: statistics-in-biosciences.csl\nbibliographystyle: apa\ncitation: true\n---\n\n\n\n# Rare events are ... rare\n\nI recently came across this document (white paper??) by Marin Bland called '[Detecting a single event](https://www-users.york.ac.uk/~mb55/bsi_study/single_event.pdf)', and wanted to learn more about the method behind the math. I don't often get to sink back into stats theory, and this is a approachable method.\n\nIn it, he says:\n\n::: {.callout-note appearance=\"minimal\"}\nThe problem is this. If we have a series of cases where no event has taken place, what is the estimated event rate? ...\\\n\\\nJust because we have not seen an event yet does not mean we will never see one.\n:::\n\nWhat he tries to provide us with is a sense of certainty around what isn't directly observed.\n\nSay, for example, I want to study the incidence of a rare event. I want to enroll a enough participants, but, because the event is rare, there is a fair chance that I will simply see no events at all during the study ‚Äì because that's how probability rolls (ü•Å).\n\nJust because I didn't see it just because it isn't there! So.... what could it have been?\n\nLet's say we know an event is likely to occur 1% of the time. Would it make sense to run a study in 5 participants? No. This is intuitive, because even under perfectly average conditions, 1% of 5 is still \\< 1, and we will never see less than 1 person obtain an event. What about N = 100? We *might* get 1 event! But.... we might not. But it feels reasonable to believe that a study of N = 100 could have 0 events and still reflect a 1% incidence rate. But does 0 events out of 100 participants feel reasonable for a 10% incidence rate?\n\n# How do we quantify \"enough\"?\n\nSo, there are two ways to frame this as questions:\n\n1.  I have N participants in my study, and I saw 0 events, what is the range of incidence rates that are *reasonable* for me assume I've captured?\n    -   This is the upper limit of the Confidence Interval.\n2.  I want to run a study, what number of participants do I need to enroll to maximize my likelihood of observing an event?\n    -   This is based on Type II error assuming the alternative is true.\n\nTo understand the method, we should revisit the binomial distribution.\n\n# The Binomial Distribution\n\nThe binomial distribution provides an estimate of the probability a total number of *n* independent events occur. For example, if I flip a coin 6 times, how many of them will land on heads? On average, half of the flips will be heads. Or, to generalize, total number of attempts \\* the probability that the coin will land on heads (p=0.50, 50/50), $n_{trials}*p(heads) = 6*0.5 = 3$ .\n\nIf I want to ask, what is the probability that I will see 3 heads *in a row*, [probability theory](https://en.wikipedia.org/wiki/Independence_(probability_theory)) says that the likelihood of two independent events occurring together is the multiplication of their probabilities, so in this case $p(heads)*p(heads)*p(heads)=p(heads)^3$ .\n\nThe [probability density function](https://en.wikipedia.org/wiki/Binomial_distribution) is a way to reliably estimate these. For the binomial distribution this is:\n\n$$\np(X = k) =     \\binom{n}{k} p^k(1-p)^{n-k}\n$$\n\nThat is, the probability that you will see $k$ number of successes out of $n$ total attempts, where the probability of that event occurring is $p$ (and the probability of that event *not* occurring is $1-p$ .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(paletteer)\nlibrary(colorspace)\n# pal <- \"lisa::OskarSchlemmer\"\npal <- \"LaCroixColoR::PeachPear\"\n# pal <- \"beyonce::X6\"\n# pal <- \"LaCroixColoR::Orange\"\n\nbackground_col <- \"#273649FF\"\nmy_theme <- theme_classic() + \n  theme(axis.text = element_text(size=12, color = 'white'), \n        axis.title = element_text(size=14, color = 'white'), \n        legend.text = element_text(size=12, color = 'white'), \n        legend.title = element_text(size=14, color = 'white'), \n        plot.caption = element_text(size=12, color = 'white'),\n        axis.line = element_line(color = 'white'),\n        panel.background = element_rect(color = background_col, fill = background_col), \n        plot.background = element_rect(color = background_col, fill = background_col), \n        legend.background = element_rect(color = background_col, fill = background_col)\n        # panel.background = element_rect(color = '#E1E1E1FF'), \n        # plot.background = element_rect(color = \"#E1E1E1FF\"), \n        )\n\ntheme_set(my_theme)\n```\n:::\n\n\n\n@fig-pdf shows the probability density function of K successes out of a given N when p = 0.5. The peak of the distribution shifts to half of the total N as N increases.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- seq(10, 50, by=10)\nks <- c(0:50)\np <- 0.5\n\nexamples <- crossing(k=ks, n=ns, p) %>%\n  filter(k <= n) %>%\n  rowwise() %>%\n  mutate(prob_k = dbinom(k, n, p)) \n\nexamples %>% \n  ggplot(aes(x=k, y=prob_k, color = factor(n))) + \n  geom_line(linewidth=1) + \n  labs(x='K succeses out of N', \n       y = 'P(x=K)', \n       color = 'Total N') + \n  scale_color_paletteer_d(palette = pal) +\n  scale_x_continuous(breaks=scales::pretty_breaks(10))\n```\n\n::: {.cell-output-display}\n![Probability density function of K successes out of a given N, when p = 0.5](index_files/figure-html/fig-pdf-1.png){#fig-pdf width=672}\n:::\n:::\n\n\n\nTo understand the total probability of seeing up to a certain number of events, we simply take the sum all the way up to the desired number of events K. This is called the Cumulative Density Function (@eq-cdf) ‚Äì that is it is the cumulative sum of the individual probabilities that a given K will be observed in a study.\\\n$$\nF(x; n, p) = P(X \\leq k) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1-p)^{n-k}\n$$ {#eq-cdf}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_n_20 <- examples %>% \n  mutate(n=factor(n)) %>%\n  filter(n==20) %>%\n  mutate(lt = k<=6, \n         sum=cumsum(prob_k))\n\nexample_n_20 %>%\n  ggplot(aes(x=k, y=prob_k, \n             color = lt, \n             fill = lt)) + \n  geom_bar(stat='identity', \n           position = 'identity', \n           alpha=0.7) + \n  labs(x='K succeses out of N', \n       y = 'P(x=K)') + \n  scale_x_continuous(breaks=scales::pretty_breaks(10))  + \n  theme(legend.position = 'none') +\n  scale_fill_manual(values=c('#E9A17CFF', \"grey\")) + \n  scale_color_manual(values=c('#E9A17CFF', \"grey\")) + \n  annotate(geom='text', \n           label = expression(P(x <= 6) == sum() ~ P(x == k[italic(i)]) == 0.0370), \n           color ='grey', \n           x=2, y=0.10, \n           size=4, \n           hjust=.4) + \n  annotate(geom='text', \n           label = expression(P(x > 6) == 1 - P(x <= 6) ~ \"=\" ~ 0.963), \n           color ='#E9A17CFF', \n           x=15, \n           y=0.10, \n           size=4, \n           hjust=0.2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n# Exact binomial confidence interval\n\nThe normal approximation isn't appropriate in the case of rare events. As you can see from @fig-pdf, as N increases, the probability density function approaches a normal distribution. However, with small N ( @fig-pdf-smalln ) we can see that the distribution starts to have a very high peak and narrow width.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- 10\nks <- c(0:10)\np <- 0.1\n\nexamples <- crossing(k=ks, n=ns, p) %>%\n  filter(k <= n) %>%\n  rowwise() %>%\n  mutate(prob_k = dbinom(k, n, p)) \n\nexamples %>% \n  ggplot(aes(x=k, y=prob_k, color = factor(n))) + \n  geom_line(linewidth=1) + \n  labs(x='K succeses out of N', \n       y = 'P(x=K)', \n       color = 'Total N', \n       caption = 'p = 0.1') + \n  scale_color_paletteer_d(palette = pal) +\n  scale_x_continuous(breaks=scales::pretty_breaks(10))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-pdf-smalln-1.png){#fig-pdf-smalln width=672}\n:::\n:::\n\n\n\nThere is a general rule of thumb for when you can use the normal approximation, which is $n*p \\gt 5$ and $n(1-p) \\gt 5$. @fig-npnq shows which combinations for a set of Ns and Ps are suitable for normal approximation. As N increases, P can start to decrease, however, for a small N, there is no such P that is sufficient for normal approximation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- seq(4, 20, by=2)\nps <- seq(0.1, 1, by=.1)\n\ncrossing(ns, ps) %>%\n  mutate(np = ns*ps, \n         nq = ns*(1-p), \n         met = factor(np > 5 & nq > 5, \n                      levels = c(TRUE, FALSE), \n                      labels = c('True', 'False'))) %>%\n  ggplot(aes(x=ns, y=ps, fill=met)) + \n  geom_tile(color = 'grey20') + \n  geom_text(aes(label=paste0(np, ' (', nq,')')), \n            color = 'grey20')  + \n  labs(x = 'N', y='P(x)', fill = 'np & n(1-p) > 5', \n       caption = '# (#) = np (n(1-p))') + \n  scale_fill_manual(values = c('#E9E4A6FF', '#E9A17CFF'))+ \n  scale_x_continuous(breaks=ns) + \n  scale_y_continuous(breaks=ps) + \n  theme(legend.position = 'top')\n```\n\n::: {.cell-output-display}\n![Distribution of Ns and Ps for which normal approximation is (not) suitable](index_files/figure-html/fig-npnq-1.png){#fig-npnq width=960}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}