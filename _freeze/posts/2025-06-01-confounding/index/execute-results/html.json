{
  "hash": "aec5eb1fb94bcdeedc45d5a6e2af7a31",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Convenient Sampling & Inconvenient Truths\"\ndescription: \"An example of how sampling biases can induce confounding.\"\nauthor:\n  - name: Jess Graves\ndate: 06-10-2025\ndate-modified: last-modified\nexecute-dir: project\ncrossref:\n  fig-title: '**Figure**'\n  tbl-title: '**Table**'\n  fig-labels: arabic\n  tbl-labels: arabic\n  title-delim: \".\"\nlink-citations: true\nexecute:\n  echo: true\n  warning: false\n  message: false\ncategories: [simulation, sampling, bias] \n# image: preview-image.png\ndraft: false  \nbibliography: references.bib\nnocite: |\n  @*\ncsl: statistics-in-biosciences.csl\nbibliographystyle: apa\ncitation: true\nformat: html\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Libraries\"}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggpubr)\nlibrary(truncnorm)\nmy_theme <- theme_classic(base_size = 14) + \n  theme(axis.title = element_text(size = 18))\ntheme_set(my_theme)\n```\n:::\n\n\n\n# One sample to rule them all\n\nWho is recruited into studies has dramatic impacts on the validity and generalizability of research. See this fun [xkcd](https://xkcd.com/2618/) comic about the subject.\n\n[![](images/clipboard-777350872.png){fig-alt=\"xkcd comic of a presenting standing at a podium with a \\\"Statistics Conference 2022\\\" sign behind them asking a room of people (assumed to be statisticians) to \\\"Raise your hand if you're familiar with selection bias\\\". All hands are raised. \\\"As you can see, it's a term most people know...\\\"\" width=\"400\"}](https://xkcd.com/2618/)\n\nThis type of sampling is called \"convenience sampling\".\n\n::: {.callout-note appearance=\"simple\"}\n[**Convenience sampling**]{.underline}: Convenience sampling defines a process of data collection from a population that is close at hand and easily accessible to researcher. Convenience sampling allows researcher to complete interviews or get responses in a cost effective way however they may criticized from selection bias because of the difference of the target population ([@rahi2017research])\n:::\n\nConvenience sampling can often, but does not always, lead to \"biased samples\" – or a recruited sample that distorts the relationships you want to actually understand.\n\nAnd biased samples lead to biased estimates (see @sec-statistical-bias for more on what \"biased estimates\" are.)\n\nNow, generally, people are not out there trying to shoot themselves in the foot by recruiting totally invalid samples for their studies. But, tight deadlines and small budgets often win over more complex sampling methods (like stratification), and so it is common to rely on convenience (or convenience-esque) recruitment methods. Plus, it is almost impossible to design a *perfect* study (hence the importance of replication).\n\n# Convenience becomes confounding\n\nOver the years I have done analyses for many observational cohort studies. And in my training, I was taught to always first explore *who* is in the data before diving into deeper analysis in order to identify any potential bias in the sampling. And very often it comes to light that there is some kind of sampling bias.\n\nOne particular way that sampling bias can impact a study is that it can *induce* confounding bias.\n\n::: {.callout-note appearance=\"simple\"}\n**Confounding** – I've also seen this referred to as the \"mixing of effects\". It is when the relationship you're trying to observe between two variables (X and Y) are mixed in with effects of another factor (Z) and distort the true relationship between your two variables of interest.\n:::\n\nSo, in the case of sampling bias what happens is that you recruit a sample and do it in such a way that you accidentally create unintended (or nonexistent) relationships between a feature of your dataset (Z, the confounding) and the variable your interested in (X) and Z and the outcome your interested in (Y).\n\n![Confounding variable Z influences X and Y](images/clipboard-782013072.png){#fig-confounding width=\"500\"}\n\n## As an example... \n\nA classic example of confounding is the finding that ice cream sales are correlated with crime rates. This is a spurious relationship that is actually the result of temperature driving ice cream sales and crime rates separately (see @sec-ice-cream-confounding)!\n\nBut, let's stick to a more relevant example to classic public health kinds of studies.\n\nLet's say we want to run a study where we want to assess stress levels in pre-teens to college age students. We expect that as students get older, their stress increases. We might also expect that females might report more stress than males on average.\n\nSo, this is our expected model of the relationships:\n\n![Theoretical model of relationships between age, sex, and stress](images/clipboard-548074874.png){#fig-theoretical-model width=\"500\"}\n\nSo, we go to recruit our cohort and do so via convenience sampling. We recruit a population of 10-25 year olds and measure their stress scores. When we look at the distribution of males in the dataset, however, we notice that we accidentally didn't recruit males who are \\> 20 years of age! Why did this happen?? Maybe drinking age males are less interested in participating in studies?? Or maybe we just didn't have that age and sex demographic group in our convenience domain.\n\nNo matter the reason, this sample results in relationships like @fig-induced-confounding instead of what we believe/know to be true in @fig-theoretical-model. Not only are males going to be younger on average (spuriously, males are not inherently younger than females (at least at this age)...), but they are also going to have lower scores on average. So being male is correlated with both our outcome and our exposure.\n\n![Biased sampling that impacts both and X and Y induces a confounding effect](images/clipboard-565088094.png){#fig-induced-confounding width=\"500\"}\n\nWhat are the impacts of a sampling bias like this? Can we correct for them in the models? What are the limitations?\n\n# Methods\n\nSo, let's do a simulation! We'll simulate a toy dataset of a study design like the one mentioned above. And we'll fit various models to see how biased or unbiased the age effects are.\n\n## Generating the study data\n\nI'm going to simulate a study where:\n\n1.  N = 1000\n2.  Sex (Z, confounder) was drawn from random 50/50 assignment ( $sex \\sim Bin(n=N, p=0.5)$ )\n3.  Age (X, the main predictor) was drawn from a truncated normal distribution\n    1.  Females was drawn from $age \\sim N(\\mu=16,\\sigma^2=10, a=10, b=25)$\n    2.  Males was drawn from the same distribution with a lower maximum age $age \\sim N(\\mu=16,\\sigma^2=10, a=10, b=20)$\n4.  The assumed relationship between X, Z and Y was as follows:\n    -   $Stress = \\beta_0 + \\beta_1Age + \\beta_2Sex + \\epsilon$\n\nwhere:\n\n$\\beta_0 = 10$ as the intercept\n\n$\\beta_1 = 0.5$ as the **true relationship** between age and stress\n\n$\\beta_2 = 3$ as the relationship between sex and stress\n\n$\\epsilon \\sim N(0, 5)$ as random noise\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Data generating process\"}\ngenerate_data <- function(n = 1000,\n                          min_age = 10,\n                          male_max_age = 20,\n                          female_max_age = 25,\n                          mean_age = 16,\n                          sd_age = 10,\n                          true_beta_age = 0.5,\n                          true_beta_sex = 3,\n                          true_beta_interaction = 0,\n                          sigma = 5) {\n  # Generate sex\n  sex <- rbinom(n, 1, 0.5)\n\n  # Generate age by sex\n  age <- if_else(sex == 1,\n    rtruncnorm(length(sex == 1),\n      a = min_age,\n      b = female_max_age,\n      mean = mean_age,\n      sd = sd_age\n    ),\n    rtruncnorm(length(sex == 0),\n      a = min_age,\n      b = male_max_age,\n      mean = mean_age,\n      sd = sd_age\n    )\n  )\n\n  # Generate outcome\n  epsilon <- rnorm(n, 0, sigma)\n  y <- 10 + true_beta_age * age +\n    true_beta_sex * sex +\n    true_beta_interaction * age * sex +\n    epsilon\n\n  # Return tibble\n  tibble(\n    y = y,\n    age = age,\n    sex = factor(sex, labels = c(\"Male\", \"Female\"))\n  )\n}\n```\n:::\n\n\n\nThis results is something that looks like this...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_data() |>\n  ggplot(aes(x = age, y = y, color = sex)) +\n  geom_point(alpha = 0.25, shape = 3) +\n  stat_smooth(method = \"lm\") +\n  scale_color_manual(\"\", values = c(\"red\", \"blue\")) +\n  theme_classic(base_size = 14) + \n  labs(x = \"Age (yrs)\", \n       y = \"Stress\")\n```\n\n::: {.cell-output-display}\n![Example study dataset with biased sampling](index_files/figure-html/fig-sample-example-1.png){#fig-sample-example width=576}\n:::\n:::\n\n\n\nWe can see the obvious lack of males in the older age groups (@fig-sample-example) . We can also see that males tend to report lower scores than females, but that the slope of age and stress is parallel between males and females (no interaction).\n\n## Testing the effects\n\nAs the researcher, I might not have perfect knowledge of the relationships here, so based on our @fig-theoretical-model, I might want to fit two different models:\n\n1.  One without sex as a covariate: $$stress=\\beta_0+\\beta_1Age+\\epsilon$$\n\n2.  One with sex as a covariate:\n\n    $$Stress = \\beta_0 + \\beta_1Age + \\beta_2Sex + \\epsilon$$\n\nModel 1 should give us a *biased estimate* of the relationship between age and stress – that is the $\\hat{\\beta_1}$ that this model gives us should not equal the true $\\beta_1 = 0.5$\n\nModel 2 should theoretically give us a *less biased* estimate for $\\hat{\\beta_1}$ , because there is no interaction effect with age and sex (we'll see what happens later if this changes).\n\nThen we'll do this a 1000 times and see what the average bias looks like.\n\n::: callout-note\nNow, for fun, I'm also going to generate an unbiased sample – that is, where males are not artificially truncated at age 20 – to illustrate that both models give us unbiased estimates of age effects when sampling is unbiased. (Though the estimate is noisier without adjustment.)\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Model fitting\"}\nfit_models <- function(n = 1000,\n                          min_age = 10,\n                          male_max_age = 15,\n                          female_max_age = 25,\n                          mean_age = 18,\n                          sd_age = 10,\n                          true_beta_age = 0.5,\n                          true_beta_sex = 5,\n                          true_beta_interaction = 0,\n                          sigma = 5) {\n  # Generate data\n  data <- generate_data(\n    n,\n    min_age,\n    male_max_age,\n    female_max_age,\n    mean_age,\n    sd_age,\n    true_beta_age,\n    true_beta_sex,\n    true_beta_interaction,\n    sigma\n  )\n\n  # Fit models\n  model1 <- lm(y ~ age, data = data)\n  model2 <- lm(y ~ age + sex, data = data)\n\n  # Extract coefficients for age\n  c(\n    unadjusted = coef(model1)[\"age\"],\n    adjusted = coef(model2)[\"age\"]\n  )\n}\n```\n:::\n\n\n\n# Results\n\n## Things are simple without interactions!\n\n@fig-results-bias shows the distribution of the estimated effects from each scenario as well as their bias (or difference between the estimated effect and the known true effect of 0.5).\n\nSampling bias such as this does in fact induce a *positive confounding* effect, where without accounting for sex we see an exaggerated effect size – it appears that stress increases almost 0.9 points per year of age, when we know that the true estimate is around half of that, 0.5 points per year.\n\nWhen we adjust for sex in a scenario where we have known sampling bias that induces confounding, we can see that the new effect size for age is *no longer biased!* Hoorraayy!\n\n::: callout-note\nSo, in a perfect world where:\n\n1.  We have perfect model specification\n2.  There are no non-linear or interaction teams present\n\nsampling bias based can be accounted for by adding the known confounder into the model!\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simulating biased and unbiased data and fitting models 1000 times\"}\nsims_df <- as_tibble(t(\n  replicate(\n    1000,\n    fit_models()\n  )\n)) |>\n  mutate(condition = \"Biased sampling\\n(No males > 20yrs old)\") |>\n  bind_rows(as_tibble(t(\n    replicate(\n      1000,\n      fit_models(male_max_age = 25)\n    )\n  )) |>\n    mutate(condition = \"Unbiased sampling\\n(Males & females have same distributions)\"))\n\nsims_df <- sims_df |>\n  pivot_longer(-condition) |>\n  mutate(bias = value - 0.5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- sims_df |> \n  ggplot(aes(x=value, fill = name)) + \n  geom_density( alpha =0.5) +\n  facet_wrap(~condition) +\n  labs(x = 'Estimated effect', \n       y = \"Stress\")+ \n  geom_vline(xintercept = 0.5, \n             linetype = 2, \n             color = 'black', \n             alpha = 0.5) + \n  annotate(geom = \"text\", \n           y = Inf, \n           x = 0.6, \n           vjust = 1.5,\n           hjust = 0.4,\n           label = \"<- True effect\")\n\np2 <- sims_df |> \n  ggplot(aes(x=bias, fill = name)) + \n  geom_density( alpha =0.5) +\n  facet_wrap(~condition) + \n  labs(x = \"Difference in observed & true effect (bias)\", \n       y = \"Stress\") + \n  geom_vline(xintercept = 0, \n             linetype = 2, \n             color = 'black', \n             alpha = 0.5)+ \n  annotate(geom = \"text\", \n           y = Inf, \n           x = 0, \n           vjust = 1.5,\n           hjust = 0,\n           label = \"<- No bias\")\n\n(p1/p2) + plot_layout(guides = \"collect\") & \n  scale_fill_manual(\"Adjusted for confounding\", values = c(\"darkgreen\", \"red\"), \n                    labels = c(\"Yes\", \n                               \"No\")) &\n  theme(legend.position = 'bottom')\n```\n\n::: {.cell-output-display}\n![Distribution of effect sizes & bias in adjusted and unadjusted analyses from biased and unbiased samples](index_files/figure-html/fig-results-bias-1.png){#fig-results-bias width=768}\n:::\n:::\n\n\n\n## Incorrect model specification makes things harder...\n\nBut... what if some of these assumptions we made weren't true? What if the underlying data generating process wasn't what we thought it was? Say we look at our data and notice our sampling bias, but the lines between male and female look basically parallel and we *don't believe* that *there should be* an interaction between age and sex. We might think, ok, I'll just go ahead and adjust for sex and call it a day.\n\nSo, our relationships actually look more like this:\n\n![](images/clipboard-2308394495.png){#fig-theoretical-interaction width=\"500\"}\n\n### Do it again but different\n\nTo see what gives us the least biased estimates we'll do the same thing as before, but let's assume that there is a *slight interaction* effect between age and sex – that is, let's say that females get more stressed sooner than males do.\n\nSo, now, the assumed data generation process is as follows:\n\n$Stress = \\beta_0 + \\beta_1Age + \\beta_2Sex + \\beta_3Age*Sex+ \\epsilon$\n\nwhere:\n\n$\\beta_0 = 10$ as the intercept\n\n$\\beta_1 = 0.5$ as the **true relationship** between age and stress\n\n$\\beta_2 = 3$ as the relationship between sex and stress\n\n$\\beta_3 = 0.2$ as the *slight* interaction between age and sex and stress – that is, the being females affords 0.2x steeper age-related increase in stress than males\n\n$\\epsilon \\sim N(0, 5)$ as random noise\n\n@fig-sample-interaction-example shows us this new sample, and we can see that as our cohort gets older stress increases more rapidly for females than it does for males, but only slightly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ngenerate_data(true_beta_interaction=0.2) |>\n  ggplot(aes(x = age, y = y, color = sex)) +\n  geom_point(alpha = 0.25, shape = 3) +\n  stat_smooth(method = \"lm\") +\n  scale_color_manual(\"\", values = c(\"red\", \"blue\")) +\n  theme_classic(base_size = 14)  + \n  labs(x = \"Age (yrs)\", \n       y = \"Stress\")\n```\n\n::: {.cell-output-display}\n![Example dataset where age and sex have interaction](index_files/figure-html/fig-sample-interaction-example-1.png){#fig-sample-interaction-example width=576}\n:::\n:::\n\n\n\nBut of course, we the researcher don't know that or don't perceive that, and so, we might just go and fit our main effects models and not account for interactions, thinking that this might give us unbiased estimates.\n\nThis would be known as model misspecification – that is the model we fit does not accurately reflect how the data was generated or the real relationships at hand.\n\n@fig-results-bias-interaction shows us the importance of *proper model specification*.\n\n1.  The bias in the unadjusted estimate of age gets *even larger*\n2.  The adjusted estimate is less biased, but *is still biased*\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Again but with interactions. Simulating biased and unbiased data and fitting models 1000 times\"}\nsims_df2 <- as_tibble(t(\n  replicate(\n    1000,\n    fit_models(true_beta_interaction=0.2)\n  )\n)) |>\n  mutate(condition = \"Biased sampling\\n(No males > 20yrs old)\") |>\n  bind_rows(as_tibble(t(\n    replicate(\n      1000,\n      fit_models(true_beta_interaction=0.2, \n                 male_max_age = 25)\n    )\n  )) |>\n    mutate(condition = \"Unbiased sampling\\n(Males & females have same distribution)\"))\n\nsims_df2 <- sims_df2 |>\n  pivot_longer(-condition) |>\n  mutate(bias = value - 0.5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np12 <- sims_df2 |> \n  ggplot(aes(x=value, fill = name)) + \n  geom_density( alpha =0.5) +\n  facet_wrap(~condition) +\n  labs(x = 'Estimated effect', \n       y = \"Stress\")+ \n  geom_vline(xintercept = 0.5, \n             linetype = 2, \n             color = 'black', \n             alpha = 0.5) + \n  annotate(geom = \"text\", \n           y = Inf, \n           x = 0.6, \n           vjust = 1.5,\n           hjust = 0.3,\n           label = \"<- True effect\")\n\np22 <- sims_df2 |> \n  ggplot(aes(x=bias, fill = name)) + \n  geom_density( alpha =0.5) +\n  facet_wrap(~condition) + \n  labs(x = \"Difference in observed & true effect (bias)\", \n       y = \"Stress\") + \n  geom_vline(xintercept = 0, \n             linetype = 2, \n             color = 'black', \n             alpha = 0.5)+ \n  annotate(geom = \"text\", \n           y = Inf, \n           x = 0, \n           vjust = 1.5,\n           hjust = 0,\n           label = \"<- No bias\")\n\n(p12/p22) + plot_layout(guides = \"collect\") & \n  scale_fill_manual(\"Adjusted for confounding\", values = c(\"darkgreen\", \"red\"), \n                    labels = c(\"Yes\", \n                               \"No\")) &\n  theme(legend.position = 'bottom')\n```\n\n::: {.cell-output-display}\n![Distribution of effect sizes & bias in adjusted and unadjusted analyses from biased and unbiased samples](index_files/figure-html/fig-results-bias-interaction-1.png){#fig-results-bias-interaction width=768}\n:::\n:::\n\n\n\n### Do it again but correctly\n\nOk, so what if we correctly specify our model? What if we add in an interaction term between age and sex?\n\n@fig-results-bias-interaction-correct show us that correct model specification does help us recover the real relationships! However.... as you'll see, the variance is much higher... and this is likely because we simply cannot get a good estimate of the real interaction from a sample of *incomplete data*.\n\nSo, to rank the models from worst (or most biased) to least we have:\n\n1.  A model with just age is the *most biased* age effect\n2.  A model with age and sex as a main effect is less biased but still biased\n3.  A model with age, sex, and age \\* sex is the least biased at estimating the age effect, however it is *much more noisy* under biased sampling\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_models_with_int <- function(n = 1000,\n                          min_age = 10,\n                          male_max_age = 15,\n                          female_max_age = 25,\n                          mean_age = 18,\n                          sd_age = 10,\n                          true_beta_age = 0.5,\n                          true_beta_sex = 5,\n                          true_beta_interaction = 0,\n                          sigma = 5) {\n  # Generate data\n  data <- generate_data(\n    n,\n    min_age,\n    male_max_age,\n    female_max_age,\n    mean_age,\n    sd_age,\n    true_beta_age,\n    true_beta_sex,\n    true_beta_interaction,\n    sigma\n  )\n\n  # Fit models\n  model1 <- lm(y ~ age, data = data)\n  model2 <- lm(y ~ age + sex, data = data)\n  model3 <- lm(y ~ age*sex, data = data)\n\n  # Extract coefficients for age\n  c(\n    unadjusted = coef(model1)[\"age\"],\n    adjusted = coef(model2)[\"age\"],\n    interaction_adjusted = coef(model3)[\"age\"]\n  )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsims_df3 <- as_tibble(t(\n  replicate(\n    1000,\n    fit_models_with_int(true_beta_interaction=0.2)\n  )\n)) |>\n  mutate(condition = \"Biased sampling\\n(No males > 20yrs old)\") |>\n  bind_rows(as_tibble(t(\n    replicate(\n      1000,\n      fit_models_with_int(true_beta_interaction=0.2, \n                 male_max_age = 25)\n    )\n  )) |>\n    mutate(condition = \"Unbiased sampling\\n(Males & females have same ages)\"))\n\nsims_df3 <- sims_df3 |>\n  pivot_longer(-condition) |>\n  mutate(bias = value - 0.5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np13 <- sims_df3 |> \n  ggplot(aes(x=value, fill = name)) + \n  geom_density( alpha =0.5) +\n  facet_wrap(~condition) +\n  labs(x = 'Estimated effect', \n       y = \"Stress\")+ \n  geom_vline(xintercept = 0.5, \n             linetype = 2, \n             color = 'black', \n             alpha = 0.5) + \n  annotate(geom = \"text\", \n           y = Inf, \n           x = 0.6, \n           vjust = 1.5,\n           hjust = 0.2,\n           label = \"<- True effect\")\n\np23 <- sims_df3 |> \n  ggplot(aes(x=bias, fill = name)) + \n  geom_density( alpha =0.5) +\n  facet_wrap(~condition) + \n  labs(x = \"Difference in observed & true effect (bias)\", \n       y = \"Stress\") + \n  geom_vline(xintercept = 0, \n             linetype = 2, \n             color = 'black', \n             alpha = 0.5)+ \n  annotate(geom = \"text\", \n           y = Inf, \n           x = 0, \n           vjust = 1.5,\n           hjust = 0,\n           label = \"<- No bias\")\n\n(p13/p23) + plot_layout(guides = \"collect\") & \n  scale_fill_manual(\"Adjusted for\", values = c(\"darkgreen\", \"blue\", \"red\"), \n                    labels = c(\"Sex only\", \n                               \"Age * Sex interaction\", \n                               \"None\")) &\n  theme(legend.position = 'bottom')\n```\n\n::: {.cell-output-display}\n![Distribution of effect sizes & bias in adjusted and unadjusted analyses from biased and unbiased samples](index_files/figure-html/fig-results-bias-interaction-correct-1.png){#fig-results-bias-interaction-correct width=672}\n:::\n:::\n\n\n\n# What did we learn?\n\nIn a perfect world where you only need to consider a few *known* variables and have perfect model specification that matches the real relationships perfectly, you *can* use models to correct for sampling bias...\n\nBut we live in a very imperfect world... And 95% of science is about *discovering relationships!* We don't often go into our analyses knowing the *exact models*.\n\nSo, remember:\n\n1.  Sampling bias can cause some *significant and sometimes insurmountable* problems for your analysis.\n    -   Recruiting for your sample is a one way door! Once you get that sample you can't go back!\n2.  Correct model specification is a huge and often unattainable assumption. But, unfortunately incorrect model specification has big implications on your estimates.\n\n::: callout-important\nIn a world full of imperfect data and messy recruitment, our best defense is thoughtful model specification — but even that has limits.\n:::\n\n# Appendices\n\n## Statistical bias {#sec-statistical-bias}\n\n[**Statistical Bias**](https://en.wikipedia.org/wiki/Bias_of_an_estimator#Definition): Systematic difference in the true effect and the one that is observed, due to factors such as improper study design, data collection, or selective reporting, which can lead to exaggerated or misleading conclusions.\n\nIn math language, this is conceptualized as the difference between the an estimated parameter, or the \"[expected value](https://en.wikipedia.org/wiki/Expected_value)\" of that parameter (e.g., the mean from a dataset we collected), and the real parameter:\n\n$$\nBias = E[\\hat{\\mu}] - \\mu_{true} \n$$\n\nEstimators, like means, have a distribution. So, just because you observe a mean that is different than the true mean doesn't mean you are observing \"bias\" necessarily. Instead, you might be just in the lower-probability area of sampling distribution.\n\nA truly biased statistical estimator is one that is consistently or systematically resulting in an estimate that is incorrect. So, on average, it's estimation differs from the true average.\n\n![Difference between estimators: an unbiased estimator theta 2 centered around theta vs. a biased estimator theta 1 .](images/clipboard-871674290.png)\n\nNow, ideally, we want to make sure that the statistic we are using to estimate a feature of the data actually reflects the true population. So, we want to minimize bias.\n\n## Confounding: did you know ice cream causes *crime?!* {#sec-ice-cream-confounding}\n\nThis classic example is used to demonstrate the fact that correlation $\\ne$ causation. But it is also a great example of confounding.\n\nThe example is about the fact that both ice cream sales and crime rates are positively correlated. Clearly, we know ice cream does not cause crime. But, what is it that ice cream sales and crime rates have in common? **Temperature**. As it's gets hotter, we buy more ice cream. And as it gets hotter, there is generally more crime (because people are out and about).\n\nAnd while this might feel quite obvious, it's important to look at every scatter plot we see at the same level of scrutiny. What *other* factors might be causing the relationship we see?\n\n![](images/clipboard-243329372.png)\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"A very run of the mill example of generating data to illustrate the relationship between ice cream and crime\"}\nset.seed(666)\nn <- 500\ntemperature <- runif(n, min = 50, max = 100)  # degrees Fahrenheit\n\n# Ice cream sales increase with temperature\nice_cream_sales <- 20 + 0.75 * temperature + rnorm(n, sd = 10)\n\n# Crime also increases with temperature\ncrime_rate <- 50 + 0.8 * temperature + rnorm(n, sd = 10)\n\n# Create a data frame\ndata <- tibble(ice_cream_sales, \n               crime_rate, \n               temperature)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Correlation plots\"}\nf1 <- data |> \n  ggplot(aes(x=ice_cream_sales, y=crime_rate)) + \n  geom_point() + \n  labs(x='Ice cream sales', \n       y=\"Crime incidents\") + \n  geom_smooth(method = 'lm', color = 'red') +\n  stat_cor(method = \"pearson\", \n           p.accuracy = 0.001,           \n           label.x.npc = 0.1, label.y.npc = 0.95, \n           color = 'red') \n\nf2 <- data |> \n  ggplot(aes(x=temperature, y=crime_rate)) + \n  geom_point() + \n  labs(x='Temperature', \n       y=\"Crime incidents\") + \n  geom_smooth(method = 'lm', color = 'red') +\n  stat_cor(method = \"pearson\", \n           p.accuracy = 0.001,\n           label.x.npc = 0.1, label.y.npc = 0.95, \n           color = 'red') \n\nf3 <- data |> \n  ggplot(aes(x=temperature, y=ice_cream_sales)) + \n  geom_point() + \n  labs(x='Temperature', \n       y=\"Ice cream sales\") + \n  geom_smooth(method = 'lm', color = 'red') +\n  stat_cor(method = \"pearson\", \n           p.accuracy = 0.001,\n           label.x.npc = 0.1, label.y.npc = 0.95, \n           color = 'red') \n\ntop_row <- f1 + f2\nbottom_row <- (plot_spacer() + f3 + plot_spacer()) + \n  plot_layout(widths = c(0.5, 1, 0.5))\n\n(top_row / bottom_row) + \n  plot_layout(heights = c(1, 1), \n              widths = c(1, 1, 1)) + \n  plot_annotation(tag_levels = \"A\", tag_suffix = \".\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Splitting correlations by temperature ranges\"}\ndata |> \n    ggplot(aes(x=ice_cream_sales, \n               y=crime_rate, \n               color = temperature)) + \n    geom_point() + \n    labs(x='Ice cream sales', \n         y=\"Crime incidents\") + \n    geom_smooth(method = 'lm', color = 'black') +\n    stat_cor(method = \"pearson\", \n             p.accuracy = 0.001,\n             label.x.npc = 0.05, label.y.npc = 0.05, \n             color = 'black') + \n  facet_wrap(~cut(temperature, \n                  breaks = c(0, 60, 70, 80, 90, 100),\n                  include.lowest = T, \n                  right = F, \n                  labels = c(\"<60F\", \n                             \"60F-69F\", \n                             \"70F-79F\", \n                             \"80F-89F\", \n                             \"90F-100F\"))) + \n  theme(legend.position = 'inside', \n        legend.position.inside = c(0.9, 0.2))  + \n  scale_color_gradient2(name = \"Temperature\", \n                        low = \"blue\", \n                        high = \"red\", \n                        midpoint = 70)\n```\n\n::: {.cell-output-display}\n![Scatter plots showing how the relationships between ice cream and crime rates disappears when you separate out the data based on temperature ranges](index_files/figure-html/fig-split-1.png){#fig-split width=768}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}