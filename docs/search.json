[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "A place to talk about what I am learning, have learned, or want to learn or things I am doing, have done, or want to do.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmall samples, big biases\n\n\n\nsimulation\n\n\neffect sizes\n\n\npublication bias\n\n\n\nThe influence of publication bias on estimating effect sizes.\n\n\n\nJess Graves\n\n\nMar 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens to the market in the first 2 months of a president‚Äôs term?\n\n\n\nanimated figure\n\n\ndata visualization\n\n\n\nUsing {quantmod} & {gganimate} to explore market changes under recent presidents.\n\n\n\nJess Graves\n\n\nMar 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday: Long Beach Animal Shelter\n\n\n\ntidy tuesday\n\n\ndata visualization\n\n\n\n\n\n\n\nJess Graves\n\n\nMar 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday: Racial disparities in reproductive research\n\n\n\ntidy tuesday\n\n\ndata visualization\n\n\n\n\n\n\n\nJess Graves\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStream graphs are a real scream üò±\n\n\n\ndata visualization\n\n\nkaggle\n\n\nblogging-to-learn\n\n\nR\n\n\n\nLooking at horror franchises over time using {ggstream}\n\n\n\nJess Graves\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling percent change\n\n\n\nlongitudinal data analysis\n\n\nsimulation\n\n\nemmeans\n\n\nlme4\n\n\nR\n\n\n\nEstimate percent change in longitudinal data using log-transformations and {emmeans}\n\n\n\nJess Graves\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Selected publications. Here is my Google Scholar profile for a full list of my publications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChanges in insulin, adiponectin and lipid concentrations with age are associated with frailty and reduced quality of life in dogs\n\n\n\nmetabolism\n\n\ndogs\n\n\naging\n\n\nhealthspan\n\n\n\n\nBrennen McKenzie, Matthew Peloquin, Jessica L. Graves, ‚Ä¶\n\n\nFeb 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaturated fatty acid concentrations are predictive of insulin sensitivity and beta cell compensation in dogs\n\n\n\nlongitudinal data analysis\n\n\nmetabolism\n\n\ndogs\n\n\n\n\nMatt Peloquin, Ashley Tovar, Jessica L. Graves, ‚Ä¶\n\n\nJun 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeeding dogs a high-fat diet induces metabolic changes similar to natural aging, including dyslipidemia, hyperinsulinemia, and peripheral insulin resistance\n\n\n\nlongitudinal data analysis\n\n\nmetabolism\n\n\naging\n\n\ndogs\n\n\n\n\nBrennen McKenzie, Matt Peloquin, Ashley Tovar, Jessica L. Graves, ‚Ä¶\n\n\nJun 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBody weight, gonadectomy, and other risk factors for diagnosis of osteoarthritis in companion dogs\n\n\n\nsurvival analysis\n\n\naging\n\n\ndogs\n\n\n\n\nJessica L. Graves, Brennen A. McKenzie, Zane Koch, ‚Ä¶\n\n\nNov 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPre-pandemic circadian phase predicts pandemic alcohol use among adolescents\n\n\n\nlongitudinal data analysis\n\n\ncircadian rhythms\n\n\n\n\nBrant P. Hasler, Meredith L. Wallace, Jessica L. Graves, ‚Ä¶\n\n\nApr 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating instruments for assessing healthspan: a multi-center cross-sectional study on health-related quality of life (HRQL) and frailty in the companion dog\n\n\n\ncrossectional\n\n\naging\n\n\ndogs\n\n\n\n\nFrances L. Chen, Tarini V. Ullal, Jessica L. Graves, ‚Ä¶\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying Distances between Non-elliptical Clusters to Enhance the Identification of Meaningful Emotional Reactivity Subtypes\n\n\n\nclustering\n\n\nsimulation\n\n\npsychology\n\n\n\n\nMeredith L. Wallace, Lisa M. McTeague, Jessica L. Graves, ‚Ä¶\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-reported sleep and circadian characteristics predict alcohol and cannabis use: A longitudinal analysis of the National Consortium on Alcohol and Neurodevelopment in Adolescence study\n\n\n\nlongitudinal data analysis\n\n\ngeneralized linear mixed effects models\n\n\ncircadian rhythms\n\n\n\n\nBrant A. Hasler, Jessica L. Graves, Meredith L. Wallace, ‚Ä¶\n\n\nMay 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreliminary Evidence That Circadian Alignment Predicts Neural Response to Monetary Reward in Late Adolescent Drinkers\n\n\n\ndeviation models\n\n\ncircadian rhythms\n\n\nneuroscience\n\n\nsubstance use\n\n\n\n\nBrant A. Hasler, Jessica L. Graves, Adriane M. Soehner, Meredith L. Wallace, ‚Ä¶\n\n\nFeb 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProfiles of Accelerometry-Derived Physical Activity Are Related to Perceived Physical Fatigability in Older Adults\n\n\n\nclustering\n\n\nr package\n\n\ncircadian rhythms\n\n\naging\n\n\n\n\nJessica L. Graves, Yujia (Susanna) Qiao, Kyle D. Moored, ‚Ä¶\n\n\nMar 2, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/2024-06-02-sfas-insulin-sensitivity/index.html",
    "href": "publications/2024-06-02-sfas-insulin-sensitivity/index.html",
    "title": "Saturated fatty acid concentrations are predictive of insulin sensitivity and beta cell compensation in dogs",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nChronic feeding of a high fat diet (HFD) in preclinical species induces broad metabolic dysfunction characterized by body weight gain, hyperinsulinemia, dyslipidemia and impaired insulin sensitivity. The plasma lipidome is not well characterized in dogs with HFD-induced metabolic dysfunction. We therefore aimed to describe the alterations that occur in the plasma lipid composition of dogs that are fed a HFD and examine the association of these changes with the clinical signs of metabolic dysfunction. Dogs were fed a normal diet (ND) or HFD for 12¬†weeks. Insulin sensitivity (SI) and beta cell compensation (AIRG) were assessed through an intravenous glucose tolerance test (IVGTT) and serum biochemistry was analyzed before the introduction of HFD and again after 12¬†weeks of continued ND or HFD feeding. Plasma lipidomics were conducted prior to the introduction of HFD and again at week 8 in both ND and HFD-fed dogs. 12¬†weeks of HFD feeding resulted in impaired insulin sensitivity and increased beta cell compensation measured by SI¬†(ND mean: 11.5 [mU/l]‚Äì1¬†min‚Äì1, HFD mean: 4.7 [mU/l]‚Äì1¬†min‚Äì1) and AIRG¬†(ND mean: 167.0 [mU/l]min, HFD mean: 260.2 [mU/l]min), respectively, compared to dogs fed ND over the same duration. Chronic HFD feeding increased concentrations of plasma lipid species and deleterious fatty acids compared to dogs fed a ND. Saturated fatty acid (SFA) concentrations were significantly associated with fasting insulin (R2‚Äâ=‚Äâ0.29), SI¬†(R2‚Äâ=‚Äâ0.49) and AIRG¬†(R2‚Äâ=‚Äâ0.37) in all dogs after 12¬†weeks, irrespective of diet. Our results demonstrate that chronic HFD feeding leads to significant changes in plasma lipid composition and fatty acid concentrations associated with metabolic dysfunction. High SFA concentrations may be predictive of deteriorated insulin sensitivity in dogs.\n\n\n\n\n\n\nCitationBibTeX citation:@online{peloquin2024,\n  author = {Peloquin, Matt and Tovar, Ashley and L. Graves, Jessica and\n    , ...},\n  title = {Saturated Fatty Acid Concentrations Are Predictive of Insulin\n    Sensitivity and Beta Cell Compensation in Dogs},\n  date = {2024-06-02},\n  url = {https://www.nature.com/articles/s41598-024-63373-5},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPeloquin, Matt, Ashley Tovar, Jessica L. Graves, and... 2024.\n‚ÄúSaturated Fatty Acid Concentrations Are Predictive of Insulin\nSensitivity and Beta Cell Compensation in Dogs.‚Äù June 2, 2024. https://www.nature.com/articles/s41598-024-63373-5."
  },
  {
    "objectID": "publications/2023-04-11-circadian-phase/index.html",
    "href": "publications/2023-04-11-circadian-phase/index.html",
    "title": "Pre-pandemic circadian phase predicts pandemic alcohol use among adolescents",
    "section": "",
    "text": "Read full article here.\n\nSummary\nLater circadian timing during adolescence is linked to worse sleep, more severe depression and greater alcohol involvement, perhaps due to circadian misalignment imposed by early school schedules. School schedules shifted later during the COVID-19 pandemic, ostensibly reducing circadian misalignment and potentially mitigating problems with depression and alcohol. We used the pandemic as a natural experiment to test whether adolescent drinkers with later circadian timing showed improvements in sleep, depression and alcohol involvement. Participants were 42 adolescents reporting alcohol use. We assessed circadian phase via dim light melatonin onset prior to the pandemic, then conducted remote assessments of sleep, depressive symptoms and alcohol use during the pandemic. Mixed-effects models were used to test for pandemic effects, covarying for age, sex, time since baseline evaluation, and current school/work status. Adolescents with later circadian timing reported less sleep than other teens on school nights, both before and during the pandemic. Although school night sleep increased during the pandemic (F‚Äâ=‚Äâ28.36,¬†p‚Äâ&lt;‚Äâ0.001), those increases were not greater for individuals with later circadian timing. Individuals with later circadian timing reported larger increases in alcohol use than other teens during the pandemic (X2‚Äâ=‚Äâ36.03,¬†p‚Äâ&lt;‚Äâ0.001). Depressive symptoms increased during the pandemic (X2‚Äâ=‚Äâ46.51,¬†p‚Äâ&lt;‚Äâ0.001) but did not differ based on circadian timing. Consistent with prior reports, adolescents with later circadian timing obtained less sleep, and later school schedules facilitated increased sleep duration. Nonetheless, individuals with later circadian timing reported the sharpest increases in alcohol use, suggesting that circadian timing contributes to risk for alcohol use beyond the effects of insufficient sleep.\n\n\n\n\n\n\nCitationBibTeX citation:@online{p._hasler2023,\n  author = {P. Hasler, Brant and L. Wallace, Meredith and L. Graves,\n    Jessica and , ...},\n  title = {Pre-Pandemic Circadian Phase Predicts Pandemic Alcohol Use\n    Among Adolescents},\n  date = {2023-04-11},\n  url = {https://www.nature.com/articles/s41598-024-63373-5},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nP. Hasler, Brant, Meredith L. Wallace, Jessica L. Graves, and... 2023.\n‚ÄúPre-Pandemic Circadian Phase Predicts Pandemic Alcohol Use Among\nAdolescents.‚Äù April 11, 2023. https://www.nature.com/articles/s41598-024-63373-5."
  },
  {
    "objectID": "publications/2023-01-18-clustering/index.html",
    "href": "publications/2023-01-18-clustering/index.html",
    "title": "Quantifying Distances between Non-elliptical Clusters to Enhance the Identification of Meaningful Emotional Reactivity Subtypes",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nCoordinated emotional responses across psychophysiological and subjective indices is a cornerstone of adaptive emotional functioning. Using clustering to identify cross-diagnostic subgroups with similar emotion response profiles may suggest novel underlying mechanisms and treatments. However, many psychophysiological measures are non-normal even in homogenous samples, and over-reliance on traditional elliptical clustering approaches may inhibit the identification of meaningful subgroups. Finite mixture models that allow for non-elliptical cluster distributions is an emerging methodological field that may overcome this hurdle. Furthermore, succinctly quantifying pairwise cluster separation could enhance the clinical utility of the clustering solutions. However, a comprehensive examination of distance measures in the context of elliptical and non-elliptical model-based clustering is needed to provide practical guidance on the computation, benefits, and disadvantages of existing measures. We summarize several measures that can quantify the multivariate distance between two clusters and suggest practical computational tools. Through a simulation study, we evaluate the measures across three scenarios that allow for clusters to differ in location, scale, skewness, and rotation. We then demonstrate our approaches using psychophysiological and subjective responses to emotional imagery captured through the Transdiagnostic Anxiety Study. Finally, we synthesize findings to provide guidance on how to use distance measures in clustering applications.\n\n\n\n\n\n\nCitationBibTeX citation:@online{l._wallace2023,\n  author = {L. Wallace, Meredith and M. McTeague, Lisa and L. Graves,\n    Jessica and , ...},\n  title = {Quantifying {Distances} Between {Non-elliptical} {Clusters}\n    to {Enhance} the {Identification} of {Meaningful} {Emotional}\n    {Reactivity} {Subtypes}},\n  date = {2023-01-18},\n  url = {https://www.tandfonline.com/doi/full/10.1080/26941899.2022.2157349},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nL. Wallace, Meredith, Lisa M. McTeague, Jessica L. Graves, and... 2023.\n‚ÄúQuantifying Distances Between Non-Elliptical Clusters to Enhance\nthe Identification of Meaningful Emotional Reactivity Subtypes.‚Äù\nJanuary 18, 2023. https://www.tandfonline.com/doi/full/10.1080/26941899.2022.2157349."
  },
  {
    "objectID": "publications/2022-02-15-circadian-reward/index.html",
    "href": "publications/2022-02-15-circadian-reward/index.html",
    "title": "Preliminary Evidence That Circadian Alignment Predicts Neural Response to Monetary Reward in Late Adolescent Drinkers",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nBackground:¬†Robust evidence links sleep and circadian rhythm disturbances to alcohol use and alcohol-related problems, with a growing literature implicating reward-related mechanisms. However, the extant literature has been limited by cross-sectional designs, self-report or behavioral proxies for circadian timing, and samples without substantive alcohol use. Here, we employed objective measures of sleep and circadian rhythms, and an intensive prospective design, to assess whether circadian alignment predicts the neural response to reward in a sample of late adolescents reporting regular alcohol use.\nMethods:¬†Participants included 31 late adolescents (18‚Äì22 y/o; 19 female participants) reporting weekly alcohol use. Participants completed a 14-day protocol including pre- and post-weekend (Thursday and Sunday) circadian phase assessments¬†via¬†the dim light melatonin onset (DLMO), in counterbalanced order. Sleep-wake timing was assessed¬†via¬†actigraphy. Circadian alignment was operationalized as the DLMO-midsleep interval; secondary analyses considered social jet lag based on weekday-weekend differences in midsleep or DLMO. Neural response to reward (anticipation and outcome) was assessed¬†via¬†a monetary reward fMRI task (Friday and Monday scans). Alcohol use was assessed at baseline and¬†via¬†ecological momentary assessment. Mean BOLD signal was extracted from two regions-of-interest (striatum and medial prefrontal cortex, mPFC) for analyses in regression models, accounting for age, sex, racial identity, and scan order.\nResults:¬†In primary analyses, shorter DLMO-midsleep intervals (i.e., greater misalignment) on Thursday predicted lower striatal and mPFC responses to anticipated reward, but not reward outcome, on Friday. Lower neural (striatum and mPFC) responses to anticipated reward on Friday correlated with more binge-drinking episodes at baseline, but were not associated with alcohol use in the post-scan weekend. In secondary analyses, greater social jet lag (particularly larger weekend delays in midsleep or DLMO) was associated with lower neural responses to reward anticipation on Monday.\nConclusion:¬†Findings provide preliminary evidence of proximal associations between objectively determined circadian alignment and the neural response to anticipated monetary reward, which is linked in turn to patterns of problematic drinking. Replication in a larger sample and experimental designs will be important next steps to determining the extent to which circadian misalignment influences risk for alcohol involvement¬†via¬†alterations in reward function.\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._hasler2022,\n  author = {A. Hasler, Brant and L. Graves, Jessica and M. Soehner,\n    Adriane and L. Wallace, Meredith and , ...},\n  title = {Preliminary {Evidence} {That} {Circadian} {Alignment}\n    {Predicts} {Neural} {Response} to {Monetary} {Reward} in {Late}\n    {Adolescent} {Drinkers}},\n  date = {2022-02-15},\n  url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.803349/full},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. Hasler, Brant, Jessica L. Graves, Adriane M. Soehner, Meredith L.\nWallace, and... 2022. ‚ÄúPreliminary Evidence That Circadian\nAlignment Predicts Neural Response to Monetary Reward in Late Adolescent\nDrinkers.‚Äù February 15, 2022. https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.803349/full."
  },
  {
    "objectID": "publications/2023-11-28-banfield/index.html",
    "href": "publications/2023-11-28-banfield/index.html",
    "title": "Body weight, gonadectomy, and other risk factors for diagnosis of osteoarthritis in companion dogs",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nObjective:¬†The aim of this study is to evaluate age, sex, body weight, breed, neuter status, and age at neutering as risk factors for diagnosis of osteoarthritis in companion dogs.\nAnimals:¬†Dogs seen as patients at Banfield Pet Hospital in the United States from 1998 to 2019 with a date of death in 2019. The final cohort consisted of 131,140 dogs.\nMethods:¬†In this retrospective cohort study, Cox proportional hazard models were used to test for associations between osteoarthritis incidence and age at baseline, sex, maximum body weight, maximum body condition score, neuter status, and age at neutering. The same model was used to test these associations in 12 representative breeds, chosen based on breed weight and sample size.\nResults:¬†Older age, higher adult body weight, gonadectomy, and younger age at gonadectomy were significantly associated with higher risks of osteoarthritis in the total cohort and in all 12 breeds evaluated. Higher body condition scores and sex were also significantly associated with osteoarthritis but with minimal effect sizes in the overall cohort, and these risk factors were not consistently significant in all breeds tested.\nClinical relevance:¬†These results will assist veterinarians in identifying dogs at higher risk for osteoarthritis and applying appropriate diagnostic, preventative, and treatment interventions. An understanding of potentially modifiable risk factors, such as body condition and neutering, will support evidence-based discussions with dog owners about risk management in individual patients.\n\n\n\n\n\n\nCitationBibTeX citation:@online{l._graves2023,\n  author = {L. Graves, Jessica and A. McKenzie, Brennen and Koch, Zane\n    and , ...},\n  title = {Body Weight, Gonadectomy, and Other Risk Factors for\n    Diagnosis of Osteoarthritis in Companion Dogs},\n  date = {2023-11-28},\n  url = {https://www.frontiersin.org/journals/veterinary-science/articles/10.3389/fvets.2023.1275964/full},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nL. Graves, Jessica, Brennen A. McKenzie, Zane Koch, and... 2023.\n‚ÄúBody Weight, Gonadectomy, and Other Risk Factors for Diagnosis of\nOsteoarthritis in Companion Dogs.‚Äù November 28, 2023. https://www.frontiersin.org/journals/veterinary-science/articles/10.3389/fvets.2023.1275964/full."
  },
  {
    "objectID": "posts/2025-02-13-streamgraph/index.html",
    "href": "posts/2025-02-13-streamgraph/index.html",
    "title": "Stream graphs are a real scream üò±",
    "section": "",
    "text": "I recently came across data-to-viz‚Äôs page on stream graphs and was inspired to learn how to use {ggstreams}, ggplot_build(), {ggrepel}, and {colorspace}."
  },
  {
    "objectID": "posts/2025-02-13-streamgraph/index.html#summary",
    "href": "posts/2025-02-13-streamgraph/index.html#summary",
    "title": "Stream graphs are a real scream üò±",
    "section": "",
    "text": "I recently came across data-to-viz‚Äôs page on stream graphs and was inspired to learn how to use {ggstreams}, ggplot_build(), {ggrepel}, and {colorspace}."
  },
  {
    "objectID": "posts/2025-02-13-streamgraph/index.html#whats-a-stream-graph",
    "href": "posts/2025-02-13-streamgraph/index.html#whats-a-stream-graph",
    "title": "Stream graphs are a real scream üò±",
    "section": "What‚Äôs a stream graph?",
    "text": "What‚Äôs a stream graph?\nFrom Wiki:\n\nStreamgraph\n\nA streamgraph, or stream graph, is a type of stacked area graph which is displaced around a central axis, resulting in a flowing, organic shape.\n\n\nHere is a pretty example that was made in R:\n\n\n\nA gorgeous stream graph on R Graph Gallery"
  },
  {
    "objectID": "posts/2025-02-13-streamgraph/index.html#code",
    "href": "posts/2025-02-13-streamgraph/index.html#code",
    "title": "Stream graphs are a real scream üò±",
    "section": "Code",
    "text": "Code\n\nSet up libraries & defaults\n\n\nCode for libraries & custom functions & themes\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggstream)\nlibrary(colorspace)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(ggpubr)\nlibrary(ggExtra)\n\n# setting ggplot theme\nmy_theme &lt;- theme_classic(base_size = 12) +\n  theme(\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16)\n  )\n\ntheme_set(my_theme)\n\n\n\n\nThe dataset\nThe data comes from a kaggle data set on horror movies. I‚Äôm going to be focusing on looking at trends across horror franchises over time.\nFirst, a tiny bit of data tidying. I‚Äôm going to:\n\nGet the dates in the right format,\nDrop Friday the 13th, Nightmare on Elm Street as a franchise because it‚Äôs a cross-over with an n of 1\nLevel the Franchises by their earliest date of release\n\n\ndf &lt;- read_csv(\"horror_movie_boxoffice.csv\") %&gt;%\n  mutate(\n    `Release Date` = as.Date(`Release Date`, format = \"%m/%d/%Y\"),\n    Year = year(`Release Date`)\n  ) %&gt;%\n  filter(Franchise !=\n    \"Friday the 13th, A Nightmare on Elm Street\") %&gt;%\n  mutate(Franchise = factor(gsub(\n    \"The Texas Chainsaw Massacre\", \"TX Chainsaw Massacre\",\n    gsub(\"Street\", \"St.\", Franchise)\n  )))\n\n# Ordering franchises by their earliest release date\norder_of_release &lt;- df %&gt;%\n  dplyr::select(Franchise, `Release Date`) %&gt;%\n  group_by(Franchise) %&gt;%\n  arrange(Franchise, `Release Date`) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  arrange(`Release Date`)\n\nlevels_by_release &lt;- order_of_release$Franchise\ndf$Franchise &lt;- factor(df$Franchise, levels = levels_by_release)\n\nThere are 20 different franchises in this dataset, with Halloween coming in first with the most films in the dataset.\n\n\nCode\nfranchises &lt;- df %&gt;%\n  group_by(Franchise) %&gt;%\n  tally() %&gt;%\n  arrange(desc(n))\n\nfranchises %&gt;%\n  mutate(Franchise = factor(Franchise, levels = franchises$Franchise)) %&gt;%\n  ggplot(aes(x = Franchise, y = n)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 25, hjust = 1)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(10)) +\n  labs(x = \"\", y = \"# of films in dataset\")\n\n\n\n\n\n\n\n\nFigure¬†1. Number of films by franchise\n\n\n\n\n\n\n\nggstream()\n\nTomato Meter\nBased on the kaggle documentation, the Tomato Meter = \"The score given by professional critics on Rotten Tomatoes.\"\nI‚Äôm going to use ggstream() to see what franchises are getting the highest ratings at what point in time.\nFor the sake of illustrating how freaking easy it is to use, I‚Äôm going to forgo any formatting for now.\n\nratings0 &lt;- df %&gt;%\n  ggplot(aes(\n    x = `Release Date`,\n    y = `Tomato Meter`,\n    fill = Franchise,\n    color = Franchise,\n    text = Franchise,\n    label = Franchise\n  )) +\n  geom_stream()\n\nratings0\n\n\n\n\n\n\n\nFigure¬†2. Boom! One line! geom_stream, you‚Äôre so easy!\n\n\n\n\n\nAlright, going to try to make it a little more aesthetically pleasing, but using {colorspace} to choose my color palette. Because the Franchises are ordered by earliest release date, I‚Äôm going to go with a sequential color palette.\n\n\nCode for setting palette\nn_franchise &lt;- length(unique(df$Franchise))\n# Palette for streams\npal &lt;- sequential_hcl(n_franchise, \"BluGrn\")\n# Palette for labels\npal2 &lt;- darken(sequential_hcl(n_franchise, \"BluGrn\"), amount = 0.2, space = \"HCL\")\n\n\n\n\nCode for color palette figure\ncolor_data &lt;- tibble(\n  color = factor(seq_along(pal)),\n  value = 1\n)\n\ncolor_data %&gt;%\n  ggplot(aes(x = color, y = value, fill = color)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = pal) +\n  theme(\n    legend.position = \"none\",\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    axis.line = element_blank(),\n    plot.title = element_text(hjust = 0.5)\n  ) +\n  labs(x = \"\", y = \"\", title = \"My custom palette\")\n\n\n\n\n\n\n\n\nFigure¬†3. Checking out what my palette will look like\n\n\n\n\n\n\n\nCode for setting new theme\nnew_theme &lt;- theme_classic() +\n  theme(\n    plot.background = element_rect(fill = \"grey95\"),\n    panel.background = element_rect(fill = \"grey95\"),\n    legend.background = element_rect(fill = \"grey95\"),\n    legend.text = element_text(size = 12),\n    axis.line = element_line(color = \"grey80\"),\n    axis.ticks = element_line(color = \"grey80\"),\n    axis.text = element_text(color = \"grey50\", size = 14),\n    axis.title = element_text(color = \"grey50\", size = 18),\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank()\n  )\ntheme_set(new_theme)\n\n\nTime to apply the formatting.\n\n\nCode for plot\nratings &lt;- ratings0 +\n  scale_y_continuous(breaks = scales::pretty_breaks(10)) +\n  scale_x_date(\n    breaks = \"5 year\", date_labels = \"%Y\",\n    expand = c(0, 0)\n  ) +\n  scale_fill_manual(name = \"\", values = pal) +\n  scale_color_manual(name = \"\", values = pal) +\n  theme(\n    legend.position = \"top\",\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  guides(fill = guide_legend(nrow = 3))\n\nratings\n\n\n\n\n\n\n\n\nFigure¬†4. Tomatometer score for each franchise over time with formatting\n\n\n\n\n\nWith 20 different levels, the legend can be a bit difficult to visually map onto the stream colors. So, let‚Äôs check out geom_stream_label(), which is a nice built in function that adds labels.\n\nratings_labels &lt;- ratings +\n  geom_stream_label(\n    fontface = \"bold\",\n    hjust = 0.25,\n    vjust = -0.5,\n    size = 5,\n    color = pal2\n  ) +\n  theme(legend.position = \"none\")\n\nratings_labels\n\n\n\n\n\n\n\nFigure¬†5. Tomatometer score for each franchise over time with labels\n\n\n\n\n\nNot too bad ‚Äì but I found it a bit hard to get the labels where I wanted with this many levels. And I‚Äôm not totally sure why the labels are being put where they are.\nTo try to get a little bit more customization in the labeling, I‚Äôm going to use ggplot_build() to get the data that‚Äôs generated from the geom_stream_label() call and then modify it. (The reasons I‚Äôm calling it after geom_stream_label() is because the transformation into stream-space is done within the geom_stream_label().)\n\n\nCode to get label data for plot\n# Earliest release data\nearliest_release &lt;- order_of_release %&gt;%\n  ungroup() %&gt;%\n  rename(label = Franchise)\n\n# Get the label data\nratings_data &lt;- ggplot_build(ratings_labels)$data[[1]] %&gt;%\n  as_tibble() %&gt;%\n  rename(x0 = x)\n\n# Transforming the stream-data back to date data\nmin_x &lt;- min(ratings_data$x0)\ntarget_date &lt;- min(df$`Release Date`)\norigin_date &lt;- target_date - min_x\nratings_data$x &lt;- as.Date(ratings_data$x0, origin = origin_date)\n\n# Combining the rating data with the earliest release data\nrd &lt;- ratings_data %&gt;%\n  left_join(., earliest_release) %&gt;%\n  mutate(dist_release = abs(x - `Release Date`))\n\n# The dates don't line up perfectly, so finding the x values in the label dataset are closest to the initial release date \nrd &lt;- rd %&gt;%\n  group_by(label) %&gt;%\n  filter(dist_release == min(dist_release)) %&gt;%\n  # Finding the middle of each stream\n  mutate(midpoint = median(y)) %&gt;% \n  dplyr::select(label, x, midpoint) %&gt;%\n  unique()\n\n\n\n\nCode to get label data to add to plot\nratings_repels &lt;- ratings +\n  geom_label_repel(rd,\n    mapping = aes(\n      x = x,\n      y = midpoint,\n      color = label,\n      label = label,\n      fill = label\n    ),\n    inherit.aes = F,\n    segment.color = NA,\n    box.padding = 0.35, # Adjust the padding inside the box\n    point.padding = 0.5, # Space between the label and the point\n    min.segment.length = 0,\n    size = 5,\n    max.overlaps = 11,\n    color = \"white\"\n  ) +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = pal)\n\nratings_repels\n\n\n\n\n\n\n\n\nFigure¬†6\n\n\n\n\n\n\n\nNumber of films in each franchise over time\nTomato Meter tells us what Franchise was getting the most ratings when. Let‚Äôs switch gears to look at which Franchises dominated the horror market in terms of sheer total number of films over time. We‚Äôll be able to see where Franchises have rapid expansion or stay stable over time.\nI‚Äôm going to pick a different palette for fun.\n\n\nCode to set base plot to add labels to\n# Palette\npal &lt;- sequential_hcl(n_franchise, \"SunsetDark\")\n\nn_films &lt;- df %&gt;%\n  dplyr::select(Franchise, `Release Date`) %&gt;%\n  group_by(Franchise) %&gt;%\n  arrange(Franchise, `Release Date`) %&gt;%\n  mutate(`N of films` = row_number()) %&gt;%\n  ggplot(aes(\n    x = `Release Date`,\n    y = `N of films`,\n    fill = Franchise,\n    color = Franchise,\n    label = Franchise\n  )) +\n  geom_stream() +\n  scale_y_continuous(breaks = scales::pretty_breaks(10)) +\n  scale_x_date(\n    breaks = \"5 year\", date_labels = \"%Y\",\n    expand = c(0, 0)\n  ) +\n  scale_fill_manual(name = \"\", values = pal) +\n  scale_color_manual(name = \"\", values = pal) +\n  theme(legend.position = \"none\") +\n  guides(fill = guide_legend(nrow = 3))\n\n\n\n\nCode to get label data\n# Get the label data\nns_data &lt;- ggplot_build(n_films)$data[[1]] %&gt;%\n  as_tibble() %&gt;%\n  rename(x0 = x)\n\nmin_x &lt;- min(ns_data$x0)\ntarget_date &lt;- min(df$`Release Date`)\norigin_date &lt;- target_date - min_x\nns_data$x &lt;- as.Date(ns_data$x0, origin = origin_date)\n\nnd &lt;- ns_data %&gt;%\n  left_join(., earliest_release) %&gt;%\n  mutate(dist_release = abs(x - `Release Date`))\n\nnd &lt;- nd %&gt;%\n  group_by(label) %&gt;%\n  filter(dist_release == min(dist_release)) %&gt;%\n  mutate(midpoint = median(y)) %&gt;%\n  dplyr::select(label, x, midpoint) %&gt;%\n  unique()\n# \n# \n# nd2 &lt;- nd %&gt;%\n#   mutate(\n#     direction = if_else(midpoint &lt; 0,\n#       min(ns_data$y),\n#       max(ns_data$y)\n#     ),\n#     dist_axis = (direction - midpoint),\n#     label_placement = midpoint + dist_axis / .8,\n#     label_placement = if_else(label_placement &lt; 0,\n#       label_placement, -1 * label_placement\n#     )\n#   )\n\n\n\n\nCode for plot\nn_films_repels &lt;- n_films +\n  geom_label_repel(nd,\n    mapping = aes(\n      x = x,\n      y = midpoint,\n      color = label,\n      label = label,\n      fill = label\n    ),\n    inherit.aes = F,\n    segment.color = NA,\n    box.padding = 0.35, # Adjust the padding inside the box\n    point.padding = 0.5, # Space between the label and the point\n    min.segment.length = 0,\n    size = 5,\n    max.overlaps = 11,\n    color = \"white\"\n  ) +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = pal)\n\nn_films_repels\n\nggsave(\"preview-image.png\", n_films_repels,\n  units = \"cm\", width = 32, height = 18\n)\n\n\n\n\n\n\n\n\nFigure¬†7. Number of films in each franchise over time with labels\n\n\n\n\n\nLove to see it. We can see how Nightmare on Elm Street emerging on the market in the late 80s, and Paranormal Activity exploding around the 2010s. And then there‚Äôs Child‚Äôs Play, consistently releasing films from the late 80s into 2020."
  },
  {
    "objectID": "posts/2025-02-13-streamgraph/index.html#and-just-for-fun-do-ratings-change-over-time",
    "href": "posts/2025-02-13-streamgraph/index.html#and-just-for-fun-do-ratings-change-over-time",
    "title": "Stream graphs are a real scream üò±",
    "section": "And, just for fun: do ratings change over time?",
    "text": "And, just for fun: do ratings change over time?\nI was curious to know if there was a general trend in ratings over time ‚Äì that is, are reviewers generally becoming more or less favorable to horror movies?\nBased on Figure¬†8, looks like no. Looks pretty flat.\n\n\nCode for scatter\n# setting ggplot theme\nmy_theme &lt;- theme_classic(base_size = 12) +\n  theme(\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16)\n  )\n\ntheme_set(my_theme)\n\np &lt;- df %&gt;%\n  dplyr::select(`Tomato Meter`, Franchise, `Release Date`) %&gt;%\n  ggplot(aes(x = `Release Date`, y = `Tomato Meter`)) +\n  geom_point() +\n  theme(legend.position = \"none\") +\n  stat_smooth(method = \"lm\") +\n  stat_cor(\n    label.y.npc = \"top\",\n    label.x.npc = \"center\"\n  )\n\nggMarginal(p,\n  type = \"histogram\",\n  xparams = list(binwidth = 365 * 2),\n  yparams = list(binwidth = 5)\n)\n\n\n\n\n\n\n\n\nFigure¬†8. Scatterplot of ratings over time\n\n\n\n\n\nBut once you look within Franchises (Figure¬†9), actually looks like most are showing some trending decrease over time. Specific yikes to Texas Chainsaw Massacre, who has tanked 50 points over it‚Äôs 38 years. And Paranormal Activity & Hellraiser, who burned hot and fast, with a 50 point decline over 6-8 years (This dataset doesn‚Äôt have the latest Hellraiser‚Ä¶).\n\n\nCode for scatter with correlations\ndf %&gt;%\n  dplyr::select(`Tomato Meter`, Franchise, `Release Date`) %&gt;%\n  ggplot(aes(\n    x = `Release Date`, y = `Tomato Meter`,\n    color = Franchise, fill = Franchise\n  )) +\n  geom_point() +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  stat_smooth(method = \"lm\", se = F) +\n  scale_fill_manual(values = pal) +\n  scale_color_manual(values = pal2) +\n  stat_cor(\n    label.x.npc = \"left\", label.y = 90,\n    size = 5\n  ) +\n  facet_wrap(~Franchise) +\n  scale_x_date(date_labels = \"%Y\", breaks = \"10 year\")\n\n\n\n\n\n\n\n\nFigure¬†9. Scatterplot of ratings over time for each franchise"
  },
  {
    "objectID": "posts/2025-02-09-percent-change/index.html",
    "href": "posts/2025-02-09-percent-change/index.html",
    "title": "Modeling percent change",
    "section": "",
    "text": "The purpose of this post is to chronicle my learnings on how to estimate percent change in longitudinal data in R using {lme4}(Bates et al. 2015) and {emmeans}(Lenth 2025)."
  },
  {
    "objectID": "posts/2025-02-09-percent-change/index.html#summary",
    "href": "posts/2025-02-09-percent-change/index.html#summary",
    "title": "Modeling percent change",
    "section": "",
    "text": "The purpose of this post is to chronicle my learnings on how to estimate percent change in longitudinal data in R using {lme4}(Bates et al. 2015) and {emmeans}(Lenth 2025)."
  },
  {
    "objectID": "posts/2025-02-09-percent-change/index.html#introduction",
    "href": "posts/2025-02-09-percent-change/index.html#introduction",
    "title": "Modeling percent change",
    "section": "Introduction",
    "text": "Introduction\nIn longitudinal studies and clinical trials, we often want to be able to compare how changes from baseline differ across groups ‚Äì specifically what the percent change from baseline is and how that differs across groups.\nIt‚Äôs tempting to re-calculate the outcome itself as percent change (\\(Y_{\\%\\Delta} = 100*\\frac{Y_t - Y_{t-1}}{Y_{t-1}}\\)) and then perform the statistical test of your choice on that new outcome. However, this isn‚Äôt a robust choice for a few reasons (which maybe warrants its own blog post? (In the meantime, see this collection of statistical myths, and other references¬†5). But, to summarize, converting your longitudinal data to a percent change score leaves you open to bias through:\n\nRegression to the mean\nMathematical coupling\nSkewness & non-normality\nHeteroscedasticity & increased variability\n\nLog-transformations to the rescue!\nIf your data are &gt; 0 you can reliably estimate percent change by simply taking the log(y) of your outcome. If your data do include 0 (but are never &lt; 0) , then you can do log(y+1) ."
  },
  {
    "objectID": "posts/2025-02-09-percent-change/index.html#method",
    "href": "posts/2025-02-09-percent-change/index.html#method",
    "title": "Modeling percent change",
    "section": "Method",
    "text": "Method\n\nSimulate some longitudinal data\nFit a linear mixed effects model to estimate group, time, and time x group effects with subjects as random intercepts, like:\n\\[\n\\begin{aligned}\nY_{it} =¬†\\beta_0 +¬†¬†\\beta_1 (\\text{Group}_i) +¬†\\beta_2 (\\text{Time}_t) + \\beta_3 (\\text{Group}_i¬†\\times¬†\\text{Time}_t) + u_i +¬†\\epsilon_{it}\n\\end{aligned}\n\\]\n\n\\(Y_{it}\\)= outcome for subject \\(i\\) & time \\(t\\)\n\\(\\beta_0\\) = overall intercept\n\\(\\beta_1\\) = fixed effect for group assignment\n\\(\\beta_2\\) = fixed effect for time\n\\(\\beta_3\\) = fixed effect for interaction between time and group assignment\n\\(u_i¬†\\sim¬†N(0,¬†\\sigma_u^2)\\) = random intercept¬†for subject \\(i\\) (accounts for individual differences)\n\\(\\epsilon_i¬†\\sim¬†N(0,¬†\\sigma_u^2)\\) = residual error\n\nUse {emmeans}(Lenth 2025) to do post-hoc comparisons to estimate percent changes & compare across groups"
  },
  {
    "objectID": "posts/2025-02-09-percent-change/index.html#code",
    "href": "posts/2025-02-09-percent-change/index.html#code",
    "title": "Modeling percent change",
    "section": "Code",
    "text": "Code\nI‚Äôm going to use the {simstudy} library to generate longitudinal data that has three timepoints across three treatment groups (Placebo, Treatment A, & Treatment B).\n\nSet up libraries & defaults\n\n\nCode for libraries & custom functions & themes\nlibrary(tidyverse)  \nlibrary(simstudy)\nlibrary(styler)\nlibrary(patchwork)\nlibrary(lme4)\nlibrary(parameters)\nlibrary(emmeans)\n\n# setting ggplot theme\nmy_theme &lt;- theme_classic() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 12), \n    strip.text = element_text(size=12)\n  )\n\ntheme_set(my_theme)\n\n# Formatting p-values for significance for tables\nformat_pvalues &lt;- function(x, stars_only = FALSE, stars = TRUE) {\n  r &lt;- ifelse(x &gt;= 0.05, \"\",\n    ifelse(x &lt; 0.001, \"***\",\n      ifelse(x &lt; 0.01 & x &gt;= 0.001, \"**\",\n        ifelse(x &gt;= 0.01 & x &lt; 0.05, \"*\", \"\")\n      )\n    )\n  )\n  p &lt;- ifelse(x &lt; 0.001, \"&lt;0.001\", format(round(x, 3), nsmall = 3))\n  rr &lt;- p\n  if (stars & !stars_only) {\n    rr &lt;- paste0(p, r)\n  }\n  if (stars_only & stars){\n    rr &lt;- r\n  }\n  rr &lt;- if_else(rr == \"NANA\" | is.na(rr), \"\", rr)\n  return(rr)\n}\n\n\n\n\nGenerating the data\nWe‚Äôre going to simulate a longitudinal dataset for a randomized control trial with 3 parallel groups with 3 total visits. I‚Äôll transform it to long format to make it easier to work with.\n\n\nCode for generating data in {simstudy}\nset.seed(1111)\ndef &lt;- defData(id = \"id\", varname = \"placeholder\", \n               formula = 0) \n# Treatment probabilities\ndef &lt;-  defDataAdd(def, varname = \"trt\", \n                  formula =  '0.33;0.33;0.34', \n                  dist = \"categorical\")  \n# Baseline value\ndef &lt;- defDataAdd(def, varname = \"baseline\", \n                  formula = 20, \n                  variance = 0.5) \n# Visit 1 values\ndef &lt;- defDataAdd(def, varname = \"visit1\", \n                  formula = \"ifelse(trt == 1, baseline, \n                  ifelse(trt == 2, baseline - 0.5, baseline - 2))\",\n                  variance = .2) \n# Visit 2 values\ndef &lt;- defDataAdd(def, varname = \"visit2\", \n                  formula = \"ifelse(trt == 1, visit1, \n                  ifelse(trt == 2, visit1 - 1, visit1 - 3))\", \n                  variance = .2) \n\n# Total N\nn &lt;- 130 \n# Generate the data\ndd_all &lt;- genData(n = n, dtDefs = def) %&gt;%\n  dplyr::select(-placeholder)\n\n# Long format\ndf &lt;- dd_all %&gt;%\n  pivot_longer(-c(1:2), names_to='time', values_to='score') %&gt;%\n  mutate(time=factor(time, labels=c('Baseline', 'Visit 1', 'Visit 2')), \n         trt = factor(trt, labels=c('Placebo', 'Treatment A', 'Treatment B')), \n         id=factor(id))\nhead(df)\n\n\n# A tibble: 6 √ó 4\n  id    trt         time     score\n  &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;dbl&gt;\n1 1     Treatment B Baseline  20.2\n2 1     Treatment B Visit 1   18.6\n3 1     Treatment B Visit 2   15.4\n4 2     Placebo     Baseline  19.4\n5 2     Placebo     Visit 1   19.8\n6 2     Placebo     Visit 2   19.8\n\n\n\nVisualizing the data\nSo, plotting the data (Figure¬†1), we can see in the data that Placebo group stays stable, Treatment A and B both show reductions from baseline, with Treatment B showing more reductions than Treatment A.\n\n\nCode for spaghetti plot\nfig1 &lt;- df %&gt;% \n  ggplot(aes(x=time, y=score, color=trt, group=trt)) + \n  geom_line(aes(group=id), aes=0.5) + \n  stat_summary(size=0.2, \n               color='black') + \n  stat_summary(color='black', geom='line') + \n  facet_wrap(~trt) + \n  scale_color_brewer(palette='Set2') + \n  theme(legend.position='none', \n        axis.text.x=element_text(angle=25, hjust=1)) + \n  scale_y_continuous(breaks=scales::pretty_breaks(10)) + \n  labs(x='')\n\nfig2 &lt;- df %&gt;% \n  ggplot(aes(x=time, y=score, color=trt, group=trt)) + \n  stat_summary(size=0.2) + \n  stat_summary(geom='line') +\n  scale_color_brewer(name='', palette='Set2') + \n  scale_y_continuous(breaks=scales::pretty_breaks(10)) + \n  labs(x='') + \n  theme(legend.position='inside', \n        legend.position.inside = c(0.2, 0.3))\n\nfig1 / fig2\n\n\n\n\n\n\n\n\nFigure¬†1. Spaghetti plot & observed means over time and treatment group\n\n\n\n\n\n\n\n\nModeling\nAs promised, we‚Äôll fit a LMM to model the data and test if these reductions are significant.\n\nmodel &lt;- lmer(log(score) ~ trt + time + trt * time + (1 | id), data = df)\n\n\n\nCode for diagnostic plots\ndiags &lt;- tibble(Fitted = fitted(model), Residuals = resid(model))\nfitted_resids &lt;- diags %&gt;%\n  ggplot(aes(x = Fitted, y = Residuals)) +\n  geom_point(alpha = 0.5, shape = 1) +\n  geom_hline(yintercept = 0) +\n  stat_smooth()\nresid_qq &lt;- diags %&gt;%\n  ggplot(aes(sample = Residuals)) +\n  geom_qq(shape = 1) +\n  geom_qq_line() +\n  labs(\n    x = \"Theoretical quantiles\",\n    y = \"Empirical quantiles\"\n  )\n\nfitted_resids | resid_qq\n\n\n\n\n\n\n\n\nFigure¬†2. Diagnostics of regression model show assumptions are largely met\n\n\n\n\n\n\n\nStatistical tests\nI personally like to use {emmeans}(Lenth 2025) to perform post-hoc statistical tests based on a fit model. Though I know there are some other great packages out there too!\nWuick sidebar on log transformations ‚Äì recall that data are modeled on the natural-log scale. Therefore, when we calculate differences of logs, they are equivalent to the ratio of logs, which can be converted into percent changes.\nPercent change = \\(100 *\\frac{a-b}{b} = 100*(\\frac{a}{b}- 1)\\)\nRecall, rules of logs say: \\(ln(a) - ln(b) = ln(\\frac{a}{b})\\)\nAs a very quick and crude example to show \\(100*ln(\\frac{a}{b}) \\approx 100*((\\frac{a}{b}) - 1)\\), let \\(a=110\\) and \\(b=100\\):\n\\[\n\\begin{aligned}\n\\text{Percent change} &\\approx \\text{Ratio of logs} \\\\\n(\\frac{a}{b}) -1 &\\approx ln(\\frac{a}{b}) \\\\\n(\\frac{110}{100}) -1 &\\approx ln(\\frac{110}{100}) \\\\\n1.1-1 &\\approx ln(1.1) \\\\\n0.10 &\\approx 0.095 \\\\\n\\end{aligned}\n\\]\n\nems &lt;- emmeans(model, ~ time | trt,\n  data = df, infer = T\n)\n\n# Calculate percent change within groups\n# ln(a) - ln(b) = ln(a/b) approx a/b - 1\npct_change &lt;- contrast(ems,\n  method = \"trt.vs.ctrl\",\n  infer = T,\n  # type = 'response' tells emmeans to present as a ratio\n  type = \"response\"\n)\n\npct_change\n\ntrt = Placebo:\n contrast           ratio      SE  df lower.CL upper.CL null t.ratio p.value\n Visit 1 / Baseline 1.006 0.00460 254    0.996    1.016    1   1.333  0.3144\n Visit 2 / Baseline 1.007 0.00461 254    0.996    1.017    1   1.463  0.2536\n\ntrt = Treatment A:\n contrast           ratio      SE  df lower.CL upper.CL null t.ratio p.value\n Visit 1 / Baseline 0.984 0.00455 254    0.974    0.994    1  -3.554  0.0009\n Visit 2 / Baseline 0.936 0.00433 254    0.926    0.945    1 -14.393  &lt;.0001\n\ntrt = Treatment B:\n contrast           ratio      SE  df lower.CL upper.CL null t.ratio p.value\n Visit 1 / Baseline 0.903 0.00404 254    0.894    0.912    1 -22.871  &lt;.0001\n Visit 2 / Baseline 0.756 0.00338 254    0.748    0.763    1 -62.651  &lt;.0001\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: dunnettx method for 2 estimates \nIntervals are back-transformed from the log scale \nP value adjustment: dunnettx method for 2 tests \nTests are performed on the log scale \n\n\nWe can interpret these ratios as: ‚ÄúFor a given reatment group, Visit 1 is [ratio] times the Baseline value‚Äù. Ratio &lt; 1 means reduction, ratio &gt; 1 means increase.\nTo convert these to percent changes, we‚Äôll just do 100*(ratio - 1).\n\nWithin group changes from baseline\nHere is some hidden code that tidy‚Äôs up the output for plotting and outputting tables ‚Äì I don‚Äôt want to force you to suffer it, but click to open if you want!\n\n\nCode for tidying the output\npct_change_tidy &lt;- pct_change %&gt;%\n  as_tibble()\n\n# Cleaning up the results for figuring & tabeling \nres &lt;- pct_change_tidy %&gt;%\n  # Converting ratio to % and formatting as character \n  mutate(across(c(ratio, lower.CL, upper.CL),\n    .fns = ~ format(round(100 * (.x - 1), 2), nsmall = 2),\n    .names = \"{.col}_pct_chr\"\n  )) %&gt;%\n  # and as numeric\n  mutate(across(c(ratio, lower.CL, upper.CL),\n    .fns = ~ (100 * (.x - 1)),\n    .names = \"{.col}_pct\"\n  )) %&gt;%\n  # Characters get concatenated so that we can reporte them more easily\n  mutate(\n    pct_change = paste0(\n      ratio_pct_chr, \" (\",\n      lower.CL_pct_chr, \", \",\n      lower.CL_pct_chr, \")\"\n    ),\n    # Adding stars for significance levels\n    stars = format_pvalues(p.value, stars_only=T),\n    p.value = format_pvalues(p.value),\n    Day = unlist(lapply(str_split(contrast, \" / \"), function(f) f[[1]]))\n  ) %&gt;%\n  # Adding in 'Baseline' data which is 0 so that we can visualize that drop from 'Baseline'\n  full_join(., crossing(\n    contrast = \"Baseline\",\n    trt = factor(unique(df$trt)),\n    ratio_pct = 0\n  )) %&gt;%\n  # Factoring & re-labeling\n  mutate(Day = factor(contrast,\n    levels = c(\n      \"Baseline\",\n      \"Visit 1 / Baseline\",\n      \"Visit 2 / Baseline\"\n    ),\n    labels = c(\"Baseline\", \"Visit 1\", \"Visit 2\")\n  )) %&gt;%\n  # Only keeping the relevant columns\n  dplyr::select(Day, trt, ratio, SE, \n                contains('pct'), \n                -contains('pct_chr'), \n                stars, p.value)\n\n\nNow we can plot the percent reductions (Figure¬†3) and put their significance values on the plot to show which groups have significant reductions from baseline.\n\n\nCode for line plot\n# Formatting significance data for plotting\nfig_stars &lt;- res %&gt;%\n  dplyr::select(trt, Day, pct_change, stars) %&gt;%\n  arrange(Day, trt) %&gt;%\n  group_by(Day) %&gt;%\n  # Adding position markers for adding stars to the figure\n  mutate(y.position = seq(0, 5, length = length(unique(df$trt)))) %&gt;%\n  filter(stars != \"ns\")\n\nfig3 &lt;- res %&gt;%\n  ggplot(aes(x = Day, y = ratio_pct, color = trt)) +\n  geom_line(aes(group = trt),\n    position = position_dodge(width = .1)\n  ) +\n  geom_errorbar(\n    aes(\n      group = trt,\n      ymin = 100 * ((ratio - SE) - 1),\n      ymax = 100 * ((ratio + SE) - 1)\n    ),\n    width = 0,\n    position = position_dodge(width = .1), alpha = 0.5\n  ) +\n  geom_point(position = position_dodge(width = .1), size = 2) +\n  scale_color_brewer(palette='Set2') +\n  scale_y_continuous(breaks = scales::pretty_breaks(8)) +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.25, 0.25)\n  ) +\n  labs(color='', x = \"\", y = \"% Change (SE)\") +\n  geom_hline(yintercept = 0, linetype = 1, color = \"black\", alpha = 0.25) +\n  # Add significance stars to figure\n  geom_text(\n    data = fig_stars,\n    aes(x = Day, y = y.position, label = stars),\n    size = 5,\n    show.legend = F,\n    fontface = \"bold\"\n  ) +\n  labs(caption = \"* = p&lt;0.05; ** = p&lt;0.01; *** = p&lt;0.001\")  \n\n\n\n\nCode for table\nlibrary(gt)\ntable &lt;- res %&gt;% \n  na.omit() %&gt;% \n  dplyr::select(trt, Day, pct_change, p.value) %&gt;%\n  rename(Treatment = trt, \n         `Comparison to baseline` = Day, \n         `% Change (95% CI)` = pct_change, \n         `p-value` = p.value) %&gt;%\n  group_by(Treatment) %&gt;%\n  gt()\n\n\nGreat! Just as we suspected! Treatment A and B do show significant reductions. Does Treatment B truly work better?\n\n\nCode combined table & line plot\nwrap_table(table, panel='full', space='free_x') + fig3\n\n\n\n\n\n\n\n\nFigure¬†3. Within group percent changes from baseline.\n\n\n\n\n\n\n\nBetween group comparisons of changes from baseline\nI personally find it easier to think about differences of percents as absolute differences and not multiplicative, so if you want to get the absolute differneces in percent change across groups:\n\nFit percent changes, just like we did above, for each post-treatment time-point,\nUse regrid() to tell {emmeans} to keep the ratio as the units we want, and\nUse contrast() to get the differences of the percent changes (not the ratio of the percent changes)\n\nüöß NOTE: This is how I personally have figured out how to do this ‚Äì if you know another way please let me know!\nAlternatively, if you‚Äôre good with ratios of ratios, you can ignore the regrid()! The significance of the results will be fairly similar, especially for larger N. For smaller N, you‚Äôll see some variability though.\nFigure¬†4 has the final results for these analyses in table + bar chart to visualize the differences.\n\n\nCode of differences in percent changes across groups\n# Grab everything but the baseline timepoint\ndays &lt;- levels(df$time)[-1]\n\n# Some list items to capture these\nday_emmeans &lt;- day_contrast &lt;- day_pairwise_groups &lt;- result &lt;- list()\nfor (i in seq_along(days)) {\n  # Group emmeans only for Baseline & Day i\n  day_emmeans[[i]] &lt;- emmeans(model, ~ time * trt,\n    data = df,\n    at = list(time = c(\n      \"Baseline\",\n      days[i]\n    ))\n  )\n\n  # Calculating percent change relative for that Day i\n  day_contrast[[i]] &lt;- contrast(day_emmeans[[i]],\n    method = \"trt.vs.ctrl\",\n    by = \"trt\",\n    adjust = \"none\",\n    type = \"response\"\n  )\n\n  # Regrid tells emmeans to keep the units here\n  # So that we can get absolute difference of the ratios\n  day_pairwise_groups[[i]] &lt;- regrid(day_contrast[[i]]) %&gt;%\n    contrast(\n      method = \"pairwise\", by = NULL,\n      infer = T\n    )\n}\n\nresults &lt;- lapply(\n  day_pairwise_groups,\n  as_tibble\n) %&gt;%\n  dplyr::bind_rows()\n\nhead(results)\n\n\n# A tibble: 6 √ó 8\n  contrast             estimate      SE    df lower.CL upper.CL t.ratio  p.value\n  &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 (Visit 1/Baseline P‚Ä¶   0.0224 0.00648  254.  0.00717   0.0377    3.47 1.80e- 3\n2 (Visit 1/Baseline P‚Ä¶   0.103  0.00612  254.  0.0889    0.118    16.9  6.11e-14\n3 (Visit 1/Baseline T‚Ä¶   0.0809 0.00609  254.  0.0666    0.0953   13.3  6.11e-14\n4 (Visit 2/Baseline P‚Ä¶   0.0712 0.00632  254.  0.0563    0.0861   11.3  7.18e-14\n5 (Visit 2/Baseline P‚Ä¶   0.251  0.00571  254.  0.238     0.265    44.0  6.11e-14\n6 (Visit 2/Baseline T‚Ä¶   0.180  0.00549  254.  0.167     0.193    32.7  6.11e-14\n\n\nWe can interpret these are the -100*estimate difference in the percent reductions from baseline across treatment groups. So, for row 1, we say Treatment A affords -100*0.0224=-2.24 more percent reduction compared to Placebo.\n\n\nCode to pretty up results for tables\nresults_to_table0 &lt;- results %&gt;%\n  mutate(\n    contrast = gsub(\"\\\\(|\\\\)\", \"\", contrast),\n    Day = map_chr(\n      str_split(contrast, \"/\"),\n      ~ .x[1]\n    ),\n    Comparison = gsub(\n      \"\\\\Visit \\\\d+/Baseline \", \"\",\n      contrast\n    ),\n    Comparison = gsub(\"-\", \"vs.\", Comparison),\n    `p-value` = format_pvalues(p.value),\n    stars = format_pvalues(p.value, stars_only = TRUE),\n    `Difference in % Change (95% CI)` =\n      paste0(\n        format(round(-100 * estimate, 2), nsmall = 2),\n        \" (\", format(round(-100 * lower.CL, 2), nsmall = 2),\n        \", \",\n        format(round(-100 * upper.CL, 2), nsmall = 2),\n        \")\"\n      )\n  )\n\nresults_to_table &lt;- results_to_table0 %&gt;%\n  dplyr::select(Day, Comparison, contains(\"95%\"), `p-value`) %&gt;%\n  mutate(across(c(Day, Comparison), factor))\n\n# gt table to put in plot\ntable_grp_diffs &lt;- results_to_table %&gt;%\n  group_by(Day) %&gt;%\n  gt() %&gt;%\n  tab_footnote(\"Tukey adjustment for pairwise comparisons\") %&gt;%\n  cols_align(align = c(\"right\"), columns = c(1, 2)) %&gt;%\n  cols_align(align = c(\"center\"), columns = 3)\n\n\n\n\nCode to create significance lines for plotting group differences\nlines &lt;- results_to_table0 %&gt;%\n  mutate(group1 = map_chr(\n    str_split(Comparison, \" vs. \"),\n    ~ .x[1]\n  )) %&gt;%\n  mutate(group2 = map_chr(\n    str_split(Comparison, \" vs. \"),\n    ~ .x[2]\n  )) %&gt;%\n  dplyr::select(\n    group1, group2, Day, \n    estimate, lower.CL, stars\n  ) %&gt;%\n  group_by(Day) %&gt;%\n  filter(stars != \"ns\") %&gt;%\n  mutate(y.position = seq(2, 8, length = n())) %&gt;%\n  ungroup() %&gt;%\n  mutate(Day = factor(Day))\n\n\n\n\nCode to create barplot to illustrate group differences\nlibrary(ggpubr)\nfigbar &lt;- res %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x = trt, y = ratio_pct, color = trt, fill = trt)) +\n  geom_bar(aes(group = trt),\n    stat = \"identity\", position = position_dodge(width = .1)\n  ) +\n  geom_errorbar(\n    aes(\n      group = trt,\n      ymin = 100 * ((ratio - SE) - 1),\n      ymax = 100 * ((ratio + SE) - 1)\n    ),\n    width = 0,\n    position = position_dodge(width = .1), alpha = 0.5\n  ) +\n  scale_color_brewer(name = \"\", palette = \"Set2\") +\n  scale_fill_brewer(name = \"\", palette = \"Set2\") +\n  theme_classic() +\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(6),\n    limits = c(-25, 8)\n  ) +\n  theme(\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 14),\n    axis.text.x = element_text(angle = 25, hjust = 1),\n    legend.text = element_text(size = 11),\n    legend.position = \"none\",\n    legend.position.inside = c(0.1, 0.25),\n    legend.background = element_blank(),\n    strip.text = element_text(size = 12)\n  ) +\n  labs(x = \"\", y = \"% Change (SE)\") +\n  geom_hline(yintercept = 0, linetype = 1, color = \"black\", alpha = 0.25) +\n  labs(caption = \"* = p&lt;0.05; ** = p&lt;0.01; *** = p&lt;0.001\") +\n  facet_wrap(~Day) +\n  # adding p-value lines from ggpubr\n  stat_pvalue_manual(\n    data = lines, label = \"stars\", xmin = \"group1\", xmax = \"group2\",\n    inherit.aes = F, tip = 0.005\n  )\n\n\n\n\nCode for final figure\ntbl_fig_grp_diffs &lt;- wrap_table(table_grp_diffs,\n  panel = \"full\", space = \"free_x\"\n) + figbar\ntbl_fig_grp_diffs\n# \n# ggsave('preview-image.png', tbl_fig_grp_diffs,\n#        units='cm',\n#        width=16*2,\n#        height=6*2)\n\n\n\n\n\n\n\n\nFigure¬†4. Group differences in changes from baseline at each timepoint"
  },
  {
    "objectID": "posts/2025-02-09-percent-change/index.html#sec-references",
    "href": "posts/2025-02-09-percent-change/index.html#sec-references",
    "title": "Modeling percent change",
    "section": "References & other articles on change scores",
    "text": "References & other articles on change scores\n\n\nBates, Douglas, Martin M√§chler, Ben Bolker, and Steve Walker. 2015. ‚ÄúFitting Linear Mixed-Effects Models Using lme4.‚Äù Journal of Statistical Software 67 (1): 1‚Äì48. https://doi.org/10.18637/jss.v067.i01.\n\n\nClifton, Lei, and David A Clifton. 2019. ‚ÄúThe Correlation Between Baseline Score and Post-Intervention Score, and Its Implications for Statistical Analysis.‚Äù Trials 20: 1‚Äì6.\n\n\nLenth, Russell V. 2025. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://rvlenth.github.io/emmeans/.\n\n\nVickers, Andrew J. 2001. ‚ÄúThe Use of Percentage Change from Baseline as an Outcome in a Controlled Trial Is Statistically Inefficient: A Simulation Study.‚Äù BMC Medical Research Methodology 1: 1‚Äì4.\n\n\nVickers, Andrew J, and Douglas G Altman. 2001. ‚ÄúAnalysing Controlled Trials with Baseline and Follow up Measurements.‚Äù Bmj 323 (7321): 1123‚Äì24."
  },
  {
    "objectID": "posts/2025-03-01-tidytuesday/index.html",
    "href": "posts/2025-03-01-tidytuesday/index.html",
    "title": "TidyTuesday: Long Beach Animal Shelter",
    "section": "",
    "text": "This week‚Äôs tidytuesday included a published dataset from Long Beach Animal Shelter. Here‚Äôs what their README says:\n\n\n\n\n\n\nThis week we‚Äôre exploring the¬†Long Beach Animal Shelter Data!\nThe dataset comes from the¬†City of Long Beach Animal Care Services¬†via the¬†{animalshelter}¬†R package.\n\nThis dataset comprises of the intake and outcome record from Long Beach Animal Shelter.\n\n\nHow has the number of pet adoptions changed over the years?\nWhich type of pets are adopted most often?\n\n\n\n\n\n\n\nWhat pets are represented in the dataset?\nWhat outcomes are most likely to occur in different pets?\nHave visit rates changed over time?"
  },
  {
    "objectID": "posts/2025-03-01-tidytuesday/index.html#what-i-hope-to-visualize",
    "href": "posts/2025-03-01-tidytuesday/index.html#what-i-hope-to-visualize",
    "title": "TidyTuesday: Long Beach Animal Shelter",
    "section": "",
    "text": "What pets are represented in the dataset?\nWhat outcomes are most likely to occur in different pets?\nHave visit rates changed over time?"
  },
  {
    "objectID": "posts/2025-03-01-tidytuesday/index.html#version-1",
    "href": "posts/2025-03-01-tidytuesday/index.html#version-1",
    "title": "TidyTuesday: Long Beach Animal Shelter",
    "section": "Version 1",
    "text": "Version 1\n\n\nCode\np_bottom &lt;- (p_hist + plot_spacer() + p_mosaic + theme(axis.title.y=element_blank())) + \n    plot_layout(nrow = 1, widths = c(2, 0.1, 1))\n(p_stream / p_bottom )+ \n  plot_layout(nrow=2, \n              heights = c(1, 0.5))"
  },
  {
    "objectID": "posts/2025-03-01-tidytuesday/index.html#version-2-final-version",
    "href": "posts/2025-03-01-tidytuesday/index.html#version-2-final-version",
    "title": "TidyTuesday: Long Beach Animal Shelter",
    "section": "Version 2 (Final version)",
    "text": "Version 2 (Final version)\n\n\nCode\nright_side &lt;- (p_hist +\n    theme(axis.text.x = element_text(vjust=1))) /\n  (p_mosaic) +\n  plot_layout(ncol = 1, \n              heights = c(1, 1))\n\n\n\n\nCode\nfinal &lt;- (p_stream | right_side) + plot_layout(widths = c(2.5, 1.1))+ \n  plot_annotation(title = 'Long Beach Animal Shelter', \n                  caption = 'Data from: Long Beach Animal Shelter (tidytuesday Week 8); Vis by: github.com/jesslgraves') & \n  theme(plot.title = element_text(size=28, face = 'bold'))\n\nfinal \n\n\n\n\n\n\n\n\n\nCode\nggsave('preview-image.png', final, \n       units='cm', \n       width = 50, \n       height = 25)"
  },
  {
    "objectID": "posts/2025-02-26-tidytuesday/index.html",
    "href": "posts/2025-02-26-tidytuesday/index.html",
    "title": "TidyTuesday: Racial disparities in reproductive research",
    "section": "",
    "text": "This week‚Äôs tidytuesday included a published dataset from review article¬†Racial and ethnic disparities in reproductive medicine in the United States: a narrative review of contemporary high-quality evidence¬†published in the¬†American Journal of Obstetrics and Gynecology¬†in January 2025. Here‚Äôs what their README says:\n\n\n\n\n\n\nThis week we‚Äôre exploring data on studies investigating racial and ethnic disparities in reproductive medicine as published in the eight highest impact peer-reviewed Ob/Gyn journals from January 1, 2010 through June 30, 2023. The data were collected as part of a review article¬†Racial and ethnic disparities in reproductive medicine in the United States: a narrative review of contemporary high-quality evidence¬†published in the¬†American Journal of Obstetrics and Gynecology¬†in January 2025.\n\n‚ÄúThere has been increasing debate around how or if race and ethnicity should be used in medical research‚Äîincluding the conceptualization of race as a biological entity, a social construct, or a proxy for racism. The objectives of this narrative review are to identify and synthesize reported racial and ethnic inequalities in obstetrics and gynecology (ob/gyn) and develop informed recommendations for racial and ethnic inequity research in ob/gyn.‚Äù\n\n\n\n\n\n\nWe know already that science has historically failed to enroll representative groups. And so, with this data I wanted to explore two things:\n\nAre there differences in representation in racial groups across study types ‚Äì that is, are we more likely to get better representation in cohort studies or clinical trials?\nAre there changes in the magnitude of representation over time ‚Äì have we seen improvements in cohort representatives over the years?"
  },
  {
    "objectID": "posts/2025-02-26-tidytuesday/index.html#what-i-hope-to-visualize",
    "href": "posts/2025-02-26-tidytuesday/index.html#what-i-hope-to-visualize",
    "title": "TidyTuesday: Racial disparities in reproductive research",
    "section": "",
    "text": "We know already that science has historically failed to enroll representative groups. And so, with this data I wanted to explore two things:\n\nAre there differences in representation in racial groups across study types ‚Äì that is, are we more likely to get better representation in cohort studies or clinical trials?\nAre there changes in the magnitude of representation over time ‚Äì have we seen improvements in cohort representatives over the years?"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html",
    "href": "posts/2025-03-09-presidents-snp/index.html",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "",
    "text": "Note\n\n\n\nEdited to include percent change analysis (See Section¬†5 ). Inspired by a comment from @louisaslett.bsky.social‚Ä¨ on bluesky.\nI also edited the title (original title: How has the market historically responded to the first 2 months of a president‚Äôs term?)"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#presidential-data",
    "href": "posts/2025-03-09-presidents-snp/index.html#presidential-data",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "Presidential data",
    "text": "Presidential data\nI have two datasets to work with:\n\nThe presidential dataset from {ggplot2}\n\nThe data(presidential) has the start and end dates for every president since Eisenhower and their party affiliation\n\nS&P 500 data from {quantmod}\n\nThis scrapes financial data during a user-specified time frame -‚Äì I chose 1980 and on. You can choose any stock or index you‚Äôre interested in. I‚Äôve chosen to use the S&P 500, which is labeled as ^GSPC (I don‚Äôt know why because I don‚Äôt know anything about ‚Äúthe market‚Äù really, but that‚Äôs just how it is).\n\n\n\n\nLibraries & themes\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(patchwork)\nlibrary(data.table)\nlibrary(colorspace)\nlibrary(ggrepel)\nlibrary(gganimate)\n# library(gifski) # can't call this due to environment issues\nlibrary(av) \nlibrary(gt)\n\nbackground &lt;- \"#FBF7F2\"\ntext &lt;- \"#5C4033\"\ntext_size &lt;- 14\ntitle_size &lt;- 16\n\nmy_theme &lt;- theme_classic() +\n  theme(\n    axis.text = element_text(size = text_size, color = text),\n    axis.title.y = element_text(\n      size = title_size, face = \"bold\", color = text,\n      margin = margin(0, 10, 0, 10)\n    ),\n    axis.title.x = element_text(\n      size = title_size, face = \"bold\", color = text,\n      margin = margin(10, 0, 10, 0)\n    ),\n    axis.line = element_line(color = text),\n    axis.ticks = element_blank(),\n    legend.text = element_text(size = text_size, color = text),\n    legend.title = element_text(size = text_size, color = text),\n    panel.background = element_rect(\n      fill = background,\n      color = background\n    ),\n    plot.background = element_rect(\n      fill = background,\n      color = background\n    ),\n    plot.title = element_text(size = 20, face = \"bold\", color = text),\n    legend.background = element_rect(\n      fill = background,\n      color = background\n    )\n  )\n\ntheme_set(my_theme)\n\n\n\ndata(presidential) # from ggplot2\npresidential %&gt;% gt()\n\n\n\n\n\n\n\nname\nstart\nend\nparty\n\n\n\n\nEisenhower\n1953-01-20\n1961-01-20\nRepublican\n\n\nKennedy\n1961-01-20\n1963-11-22\nDemocratic\n\n\nJohnson\n1963-11-22\n1969-01-20\nDemocratic\n\n\nNixon\n1969-01-20\n1974-08-09\nRepublican\n\n\nFord\n1974-08-09\n1977-01-20\nRepublican\n\n\nCarter\n1977-01-20\n1981-01-20\nDemocratic\n\n\nReagan\n1981-01-20\n1989-01-20\nRepublican\n\n\nBush\n1989-01-20\n1993-01-20\nRepublican\n\n\nClinton\n1993-01-20\n2001-01-20\nDemocratic\n\n\nBush\n2001-01-20\n2009-01-20\nRepublican\n\n\nObama\n2009-01-20\n2017-01-20\nDemocratic\n\n\nTrump\n2017-01-20\n2021-01-20\nRepublican\n\n\n\n\n\n\n\nNotably, the dataset is missing some features I care about:\n\nIt stops at Trump‚Äôs 1st term (2021)\nIt doesn‚Äôt separate out 2nd terms\n\nSo, I had to manually enter the rest of the data myself. Open this code fold at your own risk, it‚Äôs not pretty.\n\n\nBeware! manual data cleaning\npresidential &lt;- presidential %&gt;%\n  arrange(start) %&gt;%\n  rbind(\n    .,\n    tibble(\n      name = c(\n        \"Obama 2nd\",\n        \"Biden\",\n        \"Trump\",\n        \"Bush Jr. 2nd\",\n        \"Clinton 2nd\"\n      ),\n      start = as.Date(c(\n        \"2013-01-20\",\n        \"2021-01-20\",\n        \"2025-01-20\",\n        \"2005-01-20\",\n        \"1997-01-20\"\n      )),\n      end = as.Date(c(\n        \"2017-01-20\",\n        \"2025-01-20\", NA,\n        \"2009-01-20\",\n        \"2001-01-20\"\n      )),\n      party = c(\n        \"Democratic\",\n        \"Democratic\",\n        \"Republican\",\n        \"Republican\",\n        \"Democratic\"\n      )\n    )\n  )\n\npresidential$name[presidential$name == \"Bush\"] &lt;- c(\"Bush Sr.\", \"Bush Jr. 1st\")\npresidential$name[presidential$name == \"Trump\"] &lt;- c(\"Trump 1st\", \"Trump 2nd\")\npresidential$name[presidential$name == \"Clinton\"] &lt;- c(\"Clinton 1st\")\npresidential$name[presidential$name == \"Obama\"] &lt;- c(\"Obama 1st\")\npresidential$end[presidential$name == \"Clinton 1st\"] &lt;- as.Date(\"1997-01-20\")\npresidential$end[presidential$name == \"Obama 1st\"] &lt;- as.Date(\"2013-01-20\")\npresidential$end[presidential$name == \"Bush Jr. 2nd\"] &lt;- as.Date(\"2009-01-20\")\n\npresidential &lt;- presidential %&gt;% arrange(start)\n\npresidential$name &lt;- factor(presidential$name,\n  levels = presidential$name\n)\n\n\n\n\nCode\npresidential %&gt;% \n  gt()\n\n\n\n\nTable¬†1. Final presidential dataset\n\n\n\n\n\n\n\n\n\nname\nstart\nend\nparty\n\n\n\n\nEisenhower\n1953-01-20\n1961-01-20\nRepublican\n\n\nKennedy\n1961-01-20\n1963-11-22\nDemocratic\n\n\nJohnson\n1963-11-22\n1969-01-20\nDemocratic\n\n\nNixon\n1969-01-20\n1974-08-09\nRepublican\n\n\nFord\n1974-08-09\n1977-01-20\nRepublican\n\n\nCarter\n1977-01-20\n1981-01-20\nDemocratic\n\n\nReagan\n1981-01-20\n1989-01-20\nRepublican\n\n\nBush Sr.\n1989-01-20\n1993-01-20\nRepublican\n\n\nClinton 1st\n1993-01-20\n1997-01-20\nDemocratic\n\n\nClinton 2nd\n1997-01-20\n2001-01-20\nDemocratic\n\n\nBush Jr. 1st\n2001-01-20\n2009-01-20\nRepublican\n\n\nBush Jr. 2nd\n2005-01-20\n2009-01-20\nRepublican\n\n\nObama 1st\n2009-01-20\n2013-01-20\nDemocratic\n\n\nObama 2nd\n2013-01-20\n2017-01-20\nDemocratic\n\n\nTrump 1st\n2017-01-20\n2021-01-20\nRepublican\n\n\nBiden\n2021-01-20\n2025-01-20\nDemocratic\n\n\nTrump 2nd\n2025-01-20\nNA\nRepublican\n\n\n\n\n\n\n\n\n\n\nNow, I will need to match dates within the market data to each president‚Äôs terms.\nTo do this, I will expand the presidential data into a long format where there is a single column of the sequential dates in each term. I‚Äôm going to write a custom function to help me out here.\n(ü§î In retrospect, I did not need to write this function because there are no gaps between terms‚Ä¶ but alas, this problem generalizes to other messier data like these, so will include it in case it‚Äôs helpful to anyone reading).\n\n# Function to generate date sequences for each president\nexpand_dates &lt;- function(start_date, end_date) {\n  # If the end date is NA, use today's date\n  if (is.na(end_date)) {\n    end_date &lt;- Sys.Date()\n  }\n  # Generate the sequence of dates\n  seq(start_date, end_date, by = \"day\")\n}\n\n# Expand the data frame\nexpanded_df &lt;- presidential %&gt;%\n  rowwise() %&gt;%\n  mutate(date = list(expand_dates(start, end))) %&gt;%\n  unnest(date) %&gt;%\n  ungroup()\n\n\n\nCode\nexpanded_df %&gt;% \n  dplyr::select(name, party, date) %&gt;% \n  filter(name %in% c('Eisenhower', 'Kennedy')) %&gt;%\n  group_by(name) %&gt;%\n  slice(1:5) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\nparty\ndate\n\n\n\n\nEisenhower\n\n\nRepublican\n1953-01-20\n\n\nRepublican\n1953-01-21\n\n\nRepublican\n1953-01-22\n\n\nRepublican\n1953-01-23\n\n\nRepublican\n1953-01-24\n\n\nKennedy\n\n\nDemocratic\n1961-01-20\n\n\nDemocratic\n1961-01-21\n\n\nDemocratic\n1961-01-22\n\n\nDemocratic\n1961-01-23\n\n\nDemocratic\n1961-01-24"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#market-data",
    "href": "posts/2025-03-09-presidents-snp/index.html#market-data",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "Market data",
    "text": "Market data\nHere I use getSymbol() from the {quantmod} package to get S&P 500 data, starting from 1980-01-01, because I am not a masochist.\n\ngspc &lt;- getSymbols(\"^GSPC\",\n  auto.assign = FALSE,\n  from = \"1980-01-01\"\n)\n\nThis data didn‚Äôt need much cleaning, but I did notice that the gspc object doesn‚Äôt have traditional row.names‚Äìinstead you need to use index() if you want to extract the dates in the rows.\nI also need to generate a variable where presidents will have their party colors when plotted, with the added detail that more recent presidents have darker shades of that color.\n\n\nCleaning up the data and merging in president information\ngspc2 &lt;- as_tibble(gspc)\ngspc2$date &lt;- as.Date(index(gspc))\n\nexpanded_gspc &lt;- gspc2 %&gt;%\n  dplyr::select(GSPC.Close, date) %&gt;%\n  left_join(., expanded_df %&gt;%\n    dplyr::select(name, party, date, start) %&gt;%\n    rename(start_of_term = start))\n\nrecent_pres &lt;- expanded_gspc %&gt;%\n  filter(start_of_term &gt;= min(gspc2$date)) %&gt;%\n  mutate(gspc_log = log(GSPC.Close)) %&gt;%\n  group_by(name) %&gt;%\n  # creating a variable that will be used to have party colors where more recent terms have darker values\n  mutate(\n    time = as.numeric(as.Date(\"1970-01-01\") - start_of_term),\n    time_party = if_else(party == \"Republican\",\n      -1 * time / 150, # re-scaling\n      time / 200\n    )\n  )"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#now-lets-animate-it",
    "href": "posts/2025-03-09-presidents-snp/index.html#now-lets-animate-it",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "Now let‚Äôs animate it",
    "text": "Now let‚Äôs animate it\n\n\n\nAnimated trajectory of S&P 500 during each presidents first 8 weeks\n\n\n\n\nCode\n# Save the animation as a .gif file\nanim_save(\"snp-animated.gif\", p1 +\n  geom_label(aes(label = name, color = time_party)) +\n  transition_reveal(days_from_start),\n  fps = 10,\n  height = 15,\n  width = 25,\n  units = \"cm\",\n  res = 180,\n  end_pause = 30,\n  renderer = gifski_renderer()\n)\n#\n\n\n\n\nCode\n# Save the animation as a .mp4\nanim_save(\"snp-animated.mp4\", p1 +\n  geom_label(aes(label = name, color = time_party)) +\n  transition_reveal(days_from_start),\n  fps = 10,\n  height = 15,\n  width = 25,\n  units = \"cm\",\n  res = 180,\n  end_pause = 30,\n  renderer = av_renderer()\n)"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#why-does-this-work-so-well-sp-500-trends-are-log-linear",
    "href": "posts/2025-03-09-presidents-snp/index.html#why-does-this-work-so-well-sp-500-trends-are-log-linear",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "Why does this work so well? S&P 500 trends are log-linear",
    "text": "Why does this work so well? S&P 500 trends are log-linear\nIn my original posting of this, I had the instinct to report this on the natural log scale to help with readability (Figure¬†1 ). But I didn‚Äôt take away something important about the data: while the S&P is increasing over time, it‚Äôs increasing much faster now than it was in the past.\n\n\nplot code\n# label dataset\nsnp_over_time_nat &lt;- recent_pres %&gt;%\n  ggplot(aes(\n    x = date,\n    y = GSPC.Close,\n    color = time_party,\n    group = name\n  )) +\n  geom_line(linewidth = 1.5) +\n  scale_x_date(expand=c(0, 0.01),\n    date_breaks = \"5 years\",\n    date_labels = \"%Y\", \n    limits = c(as.Date('1981-01-01'), \n               as.Date('2025-12-31'))\n  ) +\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(10),\n    # limits = c(-300, 150)\n  ) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"\",\n    y = \"S&P 500 at closing\",\n    color = \"\",\n    title = \"S&P 500 since the 1980's\"\n  ) +\n  scale_color_continuous_diverging(\n    palette = \"Blue-Red\",\n    l1 = 20, l2 = 90,\n    p1 = 1, p2 = 1,\n    alpha = 1\n  ) +\n  geom_label_repel(\n    data = pres_labels,\n    aes(\n      x = start_of_term,\n      y = y.position,\n      label = name,\n      color = time_party\n    ),\n    nudge_y = .75,\n    max.overlaps = 10\n  )\n\nsnp_over_time_nat\n\nggsave(\"preview-image2.png\", snp_over_time_nat,\n  units = \"cm\",\n  height = 30*.6,\n  width = 30\n)\n\n\n\n\n\n\n\n\nFigure¬†4. S&P 500 since 1980‚Äôs to today (natural scale)\n\n\n\n\n\nThis means that the within-term variability is also much higher for more recent presidential terms. So, the range of change that was even possible for Reagan in 1981 was much smaller than the range of possible change for say Biden in 2021.\nThe S&P 500 closed at 132 on Reagan‚Äôs first day in office. He literally could not have reduced the S&P 500 by more than 132. Trump‚Äôs present day reduction is 200 points, with a starting point of ~6,000, so of course the point reductions are not an apples-to-apples comparisons.\nSo, as per one of my previous posts, I‚Äôll use log transform to estimate percent change. Log scaling is very good at dealing with this type of scaling issue."
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#percent-changes-in-the-market",
    "href": "posts/2025-03-09-presidents-snp/index.html#percent-changes-in-the-market",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "Percent changes in the market",
    "text": "Percent changes in the market\nAnd here are the results!\nFigure¬†5 shows the percent changes in the first 8 weeks of these same presidential terms. These results look quite different from the absolute reductions ( Figure¬†2) ! Now, the notable stand outs are Bush Jr‚Äôs & Obama‚Äôs 1st terms, with Trump‚Äôs 2nd term coming in third worst performing so far.\n\n\nCode\np1_pct &lt;- from_start_first_8_weeks_log %&gt;%\n  ggplot(aes(\n    x = days_from_start,\n    y = mean_change,\n    color = time_party,\n    group = name\n  )) +\n  geom_hline(\n    yintercept = 0,\n    linetype = 2,\n    color = text,\n    alpha = 0.5\n  ) +\n  geom_line(aes(group = name),\n    size = 1.5\n  ) +\n  scale_x_continuous(\n    expand = c(0, 0),\n    limits = c(7, weeks_in_office * 7),\n    breaks = seq(7, weeks_in_office * 7, by = 7),\n    labels = c(1:8)\n  ) +\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(10),\n    # limits = c(-300, 150)\n  ) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Weeks since taking office\",\n    y = \"% Change in S&P 500 since taking office\",\n    color = \"\",\n    title = \"S&P 500 during first 8 weeks of recent presidential terms\"\n  ) +\n  scale_color_continuous_diverging(\n    palette = \"Blue-Red\",\n    l1 = 20, l2 = 90,\n    p1 = 1, p2 = 1,\n    alpha = 1\n  ) \n\np1_labels_pct &lt;- p1_pct + \n  geom_label_repel(data=pres_labels_end_log,\n             aes(x=days_from_start, \n                 y=mean_change, \n                 label=name)\n             )\n\np1_labels_pct\n\nggsave('snp-pct-changes.png', p1_labels_pct,\n       units='cm', \n       width = 25, \n       height = 15)\n\n\n\n\n\n\n\n\nFigure¬†5. Percent canges in S&P 500 over the first 8 weeks of each president‚Äôs term (1980 and on)\n\n\n\n\n\nLet‚Äôs look more closely at Bush and Obama‚Äôs terms ( Figure¬†6) to see why these results look so different.\nBush Jr‚Äôs 1st term was generally considered to be the start of a recession ‚Äì whether or not that was ‚Äúhis fault‚Äù is not for me to debate because that‚Äôs outside the scope of my knowledge and this post. But, it‚Äôs clear that there were legitimate, persistent declines during his first term.\nAnd Obama‚Äôs 1st term actually started at the tail end of Bush Jr‚Äôs 1st term recession. This means that his starting value was dramatically lower than that of the presidential terms that book-end him. So, compared to his peers (?) at that time, he is (numerically) at a disadvantage.\nUnder Obama‚Äôs 1st term, we actually see improvements in the market starting at week 7. And, we can clearly see from the overall trajectory of the S&P during his two terms ( Figure¬†6 ) that the market turned around.\nSo, percent change does indeed take innto accounnt different baseline starting values very well ‚Äì whether they can be interpreted causally with a given term is a much more complicated question.\n\n\nplot code\nsnp_over_time_nat_bush_obama &lt;- recent_pres %&gt;%\n  filter(grepl(\"Bush Jr|Obama\", name)) %&gt;%\n  ggplot(aes(\n    x = date,\n    y = GSPC.Close,\n    color = time_party,\n    group = name\n  )) +\n  geom_line(linewidth = 1.5) +\n  scale_x_date(expand=c(0, 0.01),\n    date_breaks = \"2 years\",\n    date_labels = \"%Y\", \n  ) +\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(10),\n  ) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"\",\n    y = \"S&P 500 at closing\",\n    color = \"\",\n    title = \"S&P 500 during Bush Jr. and Obama's terms\"\n  ) +\n  scale_color_continuous_diverging(\n    palette = \"Blue-Red\",\n    l1 = 20, l2 = 90,\n    p1 = 1, p2 = 1,\n    alpha = 1\n  ) +\n  geom_label_repel(\n    data = pres_labels %&gt;%\n      filter(grepl('Bush Jr|Obama', name )),\n    aes(\n      x = start_of_term,\n      y = y.position,\n      label = name,\n      color = time_party\n    ),\n    nudge_y = .75,\n    max.overlaps = 10\n  )\n\nsnp_over_time_nat_bush_obama\n\n\n\n\n\n\n\n\nFigure¬†6. S&P 500 during Bush Jr‚Äôs and Obama‚Äôs terms"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#now-lets-animate-it-again",
    "href": "posts/2025-03-09-presidents-snp/index.html#now-lets-animate-it-again",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "Now let‚Äôs animate it again!",
    "text": "Now let‚Äôs animate it again!\n\n\n\nPercent changes in S&P 500 by presidential term\n\n\n\n\nCode\n# Save the animation as a .mp4\nanim_save(\"snp-animated-pct.mp4\", p1_pct +\n  geom_label(aes(label = name, color = time_party)) +\n  transition_reveal(days_from_start),\n  fps = 10,\n  height = 15,\n  width = 25,\n  units = \"cm\",\n  res = 180,\n  end_pause = 30,\n  renderer = av_renderer()\n)\n\n#| eval: false\n# Save the animation as a .gif\nanim_save(\"snp-animated-pct.gif\", p1_pct +\n  geom_label(aes(label = name, color = time_party)) +\n  transition_reveal(days_from_start),\n  fps = 10,\n  height = 15,\n  width = 25,\n  units = \"cm\",\n  res = 180,\n  end_pause = 30,\n  renderer = gifski_renderer()\n)"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#interpreting-market-data-is-complex",
    "href": "posts/2025-03-09-presidents-snp/index.html#interpreting-market-data-is-complex",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "Interpreting market data is complex",
    "text": "Interpreting market data is complex\nIf I were to interpret these data in a meaningful way I would say: ‚ÄúConsult a real expert.‚Äù\nBut, if you want my opinion here goes:\n\nMarket data is very challenging to summarize (both quatitatively and qualitatively) into a single ‚Äútake away‚Äù. The data should be explored in many ways, not only as market points, but also their relative scale, and within the broader context of American politics.\nYes. The market is falling under Trump‚Äôs 2nd term. Trump has shown the most dramatic point reduction in his first 8 weeks since Reagan.\nHowever, the magnitude of these changes are less extreme when you consider where the market started (at the highest baseline value of any president so far).\nUnder percent change, Bush Jr.¬†and Obama‚Äôs 1st terms see the worst reductions relative to their starting values\n\nIt seems to me that drops in Bush Jr.‚Äôs term is indicative of the onset of a real recession.\nBy contrast, Obama‚Äôs 1st term decline is more of an ‚Äòoutlier‚Äô here in that the reductions are carry overs from another smaller recession during Bush Jr‚Äôs 2nd term"
  },
  {
    "objectID": "posts/2025-03-09-presidents-snp/index.html#i-learned-so-much-from-making-this-post",
    "href": "posts/2025-03-09-presidents-snp/index.html#i-learned-so-much-from-making-this-post",
    "title": "What happens to the market in the first 2 months of a president‚Äôs term?",
    "section": "I learned so much from making this post!",
    "text": "I learned so much from making this post!\n\nBlogging is a great way to learn and learn from people\n\nThank you to @louisaslett.bsky.social‚Ä¨ for encouraging me to look at these data in a different way!\n\nHow to access market and presidential data\nHow to animate figures\nHow to think (more) critically about market changes\n\nThe range of market values that are possible has increased over time\nIt‚Äôs important to consider the changes in the context of both point scales and percent change\nStarting values are important to understanding the relative magnitude of changes\nHowever, starting values can also be carryovers from previous terms (Obama‚Äôs first term)"
  },
  {
    "objectID": "posts/2025-03-22-small-samples-big-effects/index.html",
    "href": "posts/2025-03-22-small-samples-big-effects/index.html",
    "title": "Small samples, big biases",
    "section": "",
    "text": "The goal of most scientific & statistical endeavors is to identify if some variable or feature influences an outcome or disease state ‚Äì and perhaps more importantly, by how much.\nThis ‚Äúhow much‚Äù is called the ‚Äútreatment effect‚Äù, or ‚Äúeffect size‚Äù more generally. I‚Äôll use effect size here. Effect sizes are the ‚Äúso what‚Äù of scientific discovery. Just because we see a difference doesn‚Äôt mean that difference is meaningful ‚Äì or, the treatment won‚Äôt dramatically change the odds of the outcome occurring.\nOne of my favorite examples is the finding that eating one hot dog will result in 36 minutes off your lifespan (Stylianou, Fulgoni III, and Jolliet 2021). No hard feelings to the authors, there‚Äôs no doubt that hot dogs are not great for your health.\nBut, for the sake of discussing the ‚Äúso what‚Äù, let‚Äôs break it down. 1 hot dog = 36 mins / 60 mins per hour / 24 hours per day / 365 days per year = 6.849315e-05 years of life. So, how many hot dogs do you need to eat to lose 1 year of life? 14,600.\nLet‚Äôs say you live to be 75 years old. Then you need to eat 195 hot dogs per year for 75 years. You‚Äôd need to be averaging one hot dog every other day for 75 years.\nüå≠ So, I won‚Äôt be changing my hot dog eating habits. I am not eating nearly enough hot dogs to make a meaningful dent in my lifespan. Plus, I love hot dogs.\nAnyway! Back to the point."
  },
  {
    "objectID": "posts/2025-03-22-small-samples-big-effects/index.html#save-a-hot-dog-for-me",
    "href": "posts/2025-03-22-small-samples-big-effects/index.html#save-a-hot-dog-for-me",
    "title": "Small samples, big biases",
    "section": "",
    "text": "The goal of most scientific & statistical endeavors is to identify if some variable or feature influences an outcome or disease state ‚Äì and perhaps more importantly, by how much.\nThis ‚Äúhow much‚Äù is called the ‚Äútreatment effect‚Äù, or ‚Äúeffect size‚Äù more generally. I‚Äôll use effect size here. Effect sizes are the ‚Äúso what‚Äù of scientific discovery. Just because we see a difference doesn‚Äôt mean that difference is meaningful ‚Äì or, the treatment won‚Äôt dramatically change the odds of the outcome occurring.\nOne of my favorite examples is the finding that eating one hot dog will result in 36 minutes off your lifespan (Stylianou, Fulgoni III, and Jolliet 2021). No hard feelings to the authors, there‚Äôs no doubt that hot dogs are not great for your health.\nBut, for the sake of discussing the ‚Äúso what‚Äù, let‚Äôs break it down. 1 hot dog = 36 mins / 60 mins per hour / 24 hours per day / 365 days per year = 6.849315e-05 years of life. So, how many hot dogs do you need to eat to lose 1 year of life? 14,600.\nLet‚Äôs say you live to be 75 years old. Then you need to eat 195 hot dogs per year for 75 years. You‚Äôd need to be averaging one hot dog every other day for 75 years.\nüå≠ So, I won‚Äôt be changing my hot dog eating habits. I am not eating nearly enough hot dogs to make a meaningful dent in my lifespan. Plus, I love hot dogs.\nAnyway! Back to the point."
  },
  {
    "objectID": "posts/2025-03-22-small-samples-big-effects/index.html#just-because-its-published-doesnt-mean-its-real",
    "href": "posts/2025-03-22-small-samples-big-effects/index.html#just-because-its-published-doesnt-mean-its-real",
    "title": "Small samples, big biases",
    "section": "Just because it‚Äôs published doesn‚Äôt mean it‚Äôs real",
    "text": "Just because it‚Äôs published doesn‚Äôt mean it‚Äôs real\nRandomized control trials (RCTs) are generally considered the gold standard, or at least a good approximation of the gold standard, for capturing and measuring real treatment effects.\nWe need accurate treatment effect estimates to:\n\nActually improve health outcomes\nGuide clinical practice\nAccurately test new treatments\n\nWe want to do the best science all the time! But RCTs have many resources and constraints to balance, like: financial and human resources, how many patients to enroll, safety, ethics (stopping trials early due to benefit or harm), regulatory risk, etc.\nAs I see it there are two major threats to accurate effect size estimation in RCTs:\n\nPublication Bias ‚Äì Journals largely publish significant results. People tend to want to read about things ‚Äúthat work‚Äù over things ‚Äúthat don‚Äôt‚Äù. The consequence of this is that journals end up only publishing a subset of the studies actually conducted on a given treatment. And not even a random subset, a systematic subset of largely significant findings. By the generally acceptable Type I error rate (5%), approximately 5% of all studies may falsely identify an effect when none exists.\n\nüìë A nice read on what publication bias means, and how to measure it (Fujian, Hooper, and Yoon 2013)\nPublication bias parallels many cognitive biases we have to combat when exploring data and interpreting results. We tend to think that if we see p &lt; 0.05, it means we have attained something real.\nThis is not specific to RCTs ‚Äì it‚Äôs a threat to all published studies.\n\nInterim Analyses (or small sample sizes) - As mentioned above, sometimes there is good motivation to stop a study early. In very high stakes trials, like cancer trials, stopping a trial early could mean reducing the time that the placebo group doesn‚Äôt receive an effective treatment. As a simplification, studies can stop early when their pre-determined statistical analysis performed at ‚Äúinterim‚Äù shows statistically significant differences. However, meta-analyses have shown that trials that stop early due to early detection of an effect often overestimates those differences, because smaller, more realistic effects take longer to reach significance.\n\nüìë I really like this paper by Bassler et al 2010 (Bassler et al. 2010), see Figure¬†1 from the paper.\nThis would apply to any prospective cohort study too\nStudy size can also be an arguable surrogate for interim analysis, because in general, they occur before a certain number of events are attained.\n\n\n\n\n\n\n\n\nFigure¬†1. Figure 3 from Bassler et al.¬†(2010) ‚Äì Stopping Randomized Trials Early for Benefit and Estimation of Treatment Effects"
  },
  {
    "objectID": "posts/2025-03-22-small-samples-big-effects/index.html#sec-questions",
    "href": "posts/2025-03-22-small-samples-big-effects/index.html#sec-questions",
    "title": "Small samples, big biases",
    "section": "How do small samples & publication bias work together to distort effect sizes?",
    "text": "How do small samples & publication bias work together to distort effect sizes?\nFirst, to make a distinction ‚Äì there are two different definitions of bias. The colloquial version, like the one used in the phrase ‚ÄúPublication Bias‚Äù, and the other is the statistical version.\n\n\n\n\n\n\nStatistical Bias: Systematic difference in the true effect and the one that is observed, due to factors such as improper study design, data collection, or selective reporting, which can lead to exaggerated or misleading conclusions.\n\n\n\nHere I‚Äôll use a simple simulation to ask how the following lead to or influence bias in observed treatment effects:\n\nSample size\nPublication bias (reporting only p &lt; 0.05 studies)\nSample size x Publication bias\n\n\n\nLibraries\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(colorspace)\nlibrary(patchwork)\n\nbackground &lt;- 'grey20'\ntext &lt;- 'grey90'\n\nmy_theme &lt;- theme_classic() + \n  theme(axis.text = element_text(size=12, \n                                 color=text), \n        axis.title = element_text(size=14, \n                                 color=text, \n                                 face= 'bold', \n                                  margin=margin(20, 20, 20, 20)), \n        legend.text = element_text(size=12, \n                                 color=text), \n        legend.title = element_text(size=14, \n                                 color=text), \n        strip.text = element_text(size=12, \n                                 color=text),\n        plot.title = element_text(color = text, \n                                  size=14, \n                                  face = 'bold'),\n        plot.subtitle = element_text(color = text, \n                                     size = 12),\n        plot.background = element_rect(color = background, \n                                       fill = background), \n        panel.background = element_rect(color = background, \n                                         fill = background),\n        legend.background = element_rect(color = background, \n                                         fill = background), \n        axis.line = element_line(color = text), \n        axis.ticks = element_blank())\n\ntheme_set(my_theme)\n\n\n\nSimulation methods\nThe simulation framework is simple. We‚Äôre going to assume that we have two groups that are different from each other. We‚Äôll:\n\nGenerate \\(N\\) observations of a random variable ( \\(y_1\\)) that is normally distributed with a mean ( \\(\\mu_1\\)) and standard deviation ( \\(\\sigma\\))\nPre-determine our effect size ( \\(\\Delta\\) )\nGenerate another set of \\(N\\) observations of another random variable ( \\(y_2\\)) that is normally distributed with a mean that is offset by our pre-determined effect size plus a little random noise ( \\(\\mu_2 = \\mu_1 + \\Delta + \\mathcal{N}(0, 0.25)\\) ) with the same standard deviation ( \\(\\sigma\\), to make our lives even simpler)\n\nThe little bit of random noise (\\(\\mathcal{N}(0, 0.25)\\)) added is meant to sort of reflect noise in ‚ú®the real world‚ú®.\n\nPerform a two-sample t-test (with equal variances), and obtain:\n\nThe observed effect size ( \\(\\hat{\\Delta}\\) ) , and it‚Äôs 95% CI\nThe p-value for that test\n\n\nRepeat üîÅ this process 100 times across a range of sample sizes.\n\nsimulate &lt;- function(mu1, mu2, sd, n_per_group){\n  y1 &lt;- rnorm(n_per_group, mu1, sd)\n  y2 &lt;- rnorm(n_per_group, mu2, sd)\n  \n  test &lt;- t.test(y1, y2, var.equal = TRUE)\n  \n  p &lt;- test$p.value\n  delta &lt;- mean(y1) - mean(y2)\n  ci &lt;- as.numeric(test$conf.int)\n  \n  results &lt;- tibble(delta = delta, \n                    lower = ci[1],\n                    upper = ci[2], \n                    pvalue = p)\n  return(results)\n}\n\n\n\nSweep set up\n# Define sweep and simulation setup\nn_sweep &lt;- seq(2, 100, by = 2)\nn_sims &lt;- 100\n\n# Create a cross grid of simulations\nresult_data &lt;- crossing(n_per_group = n_sweep, \n                        sim = 1:n_sims) %&gt;%\n  mutate(delta = NA_real_, \n         lower = NA_real_,\n         upper = NA_real_,\n         pvalue = NA_real_,\n         n = NA_integer_,\n         true_delta = NA_real_)\n\n\n\n\nSetting the simulation parameters\nNow, let‚Äôs define \\(mu_1\\), \\(\\sigma\\), and \\(\\Delta\\):\n\n\\(\\mu_1 = 5\\)\n\\(\\sigma = 2\\)\n\\(\\Delta = 1 + \\mathcal{N}(0, 0.25)\\)\n\nWe‚Äôre going to test this in sample sizes from 2‚Äì100 per group, and each run will be simulated 100 times.\n\n\nLooping over Ns\n# Perform simulations\nset.seed(666) # üòà\n# True parameters for the simulation\ndelta &lt;- 1\nmu1 &lt;- 5\nsd &lt;- 2\n\nfor(i in 1:nrow(result_data)){\n  true_delta &lt;- delta + rnorm(1, 0, 0.25)  # Introduce some variability in true_delta\n  mu2 &lt;- mu1 + true_delta  # Adjust mu2 based on the true delta\n  n_per_group &lt;- result_data$n_per_group[i]  # Number of samples per group\n  \n  sim_results &lt;- simulate(mu1, mu2, sd, n_per_group)\n  \n  result_data[i, colnames(sim_results)] &lt;- sim_results\n  result_data$n[i] &lt;- n_per_group * 2  # Total sample size (n_per_group * 2)\n  result_data$true_delta[i] &lt;- true_delta  # Store true delta\n}\n\nresult_data$bias &lt;- -1*result_data$delta - result_data$true_delta"
  },
  {
    "objectID": "posts/2025-03-22-small-samples-big-effects/index.html#results",
    "href": "posts/2025-03-22-small-samples-big-effects/index.html#results",
    "title": "Small samples, big biases",
    "section": "Results",
    "text": "Results\n\nEffects in small studies are wildly variable\nStudies with small sample sizes (small numbers of patients) produce much more variable treatment effect estimates and are therefore much more likely to show large differences when they are in fact small (Figure¬†2 ).\nNow, on average, across all studies, these effects average out to be around the true effect (1). So, if all studies on this effect were published, we could estimate that the treatment effect was around 1 by taking the average effects across all of these studies ‚Äì no matter the size of the study! So long as we have many of those studies!\nSo, to answer the first question ( Section¬†3 ) : ‚ÄúHow much does sample size influence bias in treatment effect?‚Äù. The answer, strictly based on the statistical definition of a biased estimator(which is the difference in the expected values), the answer is actually none.\n\n\n\n\n\n\nNote that this is contingent entirely on a completely unbiased selection of studies.\n\n\n\nBut the variance is incredibly high. And this variance is problematic. Because we live in a finite world, with finite studies and resources, each study is a random draw from that highly variable distribution, leading to a treatment estimate that we are inclined to believe is ‚Äúreal‚Äù.\nAnd that is where publication bias comes in to really mess things up.\n\n\nCode\npoints &lt;- result_data |&gt;\n  ggplot(aes(x=n, y=-1*delta)) + \n  geom_point(color = text, \n             alpha = 0.5, \n             position = position_jitter()) +\n  labs(x = 'Study size', \n       y='Reported effect size',\n       title = 'Small studies are on average unbiased',\n       subtitle = 'but are much more variable'\n       ) + \n  geom_hline(yintercept = 1, \n             color = 'grey10', \n             alpha=0.5, \n             linetype = 2) + \n  scale_y_continuous(breaks = scales::pretty_breaks(5)) + \n  scale_x_continuous(breaks = scales::pretty_breaks(10), \n                     expand = c(0, 0), \n                     limits = c(0, 205)) + \n  theme(plot.title = element_text(#hjust=1, \n                                  margin = margin(10, 0, 5, 0)),\n        plot.subtitle = element_text(size=12),\n        axis.line.y = element_blank())\npoints\n\n\n\n\n\n\n\n\nFigure¬†2. For small sample sizes, the observed treatment effect is highly variable and often extreme but on average reflects the true effect\n\n\n\n\n\n\n\nPublication bias + small samples = big bias\nHowever, as mentioned above, most journals do not want to publish every study because non-significant findings are not attention-grabbing enough. So, instead, what we are generally seeing in the scientific literature is only the significant studies.\nFigure¬†3 shows what treatment effects we see, assuming only the significant findings are published.\nSo, not only are results more variable in small studies, but they are more likely to be extreme values if they do in fact reach statistical significance. Increasing the study size reduces this bias, but we still see consistent over estimation even in larger sample sizes.\n\n\nAggregating results\n# Summarize the delta for each sample size (n_per_group)\ndelta_summary &lt;- result_data |&gt;\n  group_by(n_per_group, pvalue &lt; 0.05) |&gt;\n  summarize(\n    mean_delta = mean(delta),\n    sd_delta = sd(delta),\n    delta_95_ci_lower = mean_delta - 1.96 * sd_delta / sqrt(n_sims),\n    delta_95_ci_upper = mean_delta + 1.96 * sd_delta / sqrt(n_sims)\n  )\n\n\n\n\nFigure code\n# colors &lt;-  diverging_hcl(2, 'Green-Orange') \ngrns &lt;-  sequential_hcl(2, 'TealGrn') \norngs &lt;-  diverging_hcl(2, 'Green-Orange') \ncolors &lt;- c(grns[2], orngs[2])\ncolors2 &lt;-  lighten(colors, \n                    amount = 0.5)\np2 &lt;- delta_summary |&gt;\n  mutate(significance = factor(if_else(`pvalue &lt; 0.05`, \n                                       'Likely to be published (p&lt;0.05)',\n                                       'Unlikely to be published (p&gt;0.05)'), \n                               levels = c('Likely to be published (p&lt;0.05)',\n                                       'Unlikely to be published (p&gt;0.05)'))) |&gt;\n  ggplot(aes(x = n_per_group*2, \n             y = abs(mean_delta))) + \n  geom_line(aes(color = significance)) + \n  geom_ribbon(aes(ymin = abs(delta_95_ci_lower), \n                  ymax = abs(delta_95_ci_upper), \n                  fill = significance), \n              alpha=0.1)  + \n  geom_hline(yintercept = 1, \n             linetype= 2, \n             color = 'white', \n             alpha = 0.5) + \n  scale_fill_manual(values = colors2) + \n  scale_color_manual(values = colors2) +\n  scale_x_continuous(breaks = scales::pretty_breaks(10), \n                     limits = c(4, 205),\n                     expand=c(0, 0)) + \n  scale_y_continuous(breaks = scales::pretty_breaks(5), \n                     limits = c(-0.25, 4)) + \n  theme(legend.position = 'none', \n        plot.title = element_text(hjust=0, \n                                  size = 16,\n                                  margin = margin(10, 0, 10, 0)), \n        plot.subtitle = element_text(hjust=0, \n                                  size = 11,\n                                  margin = margin(0, 0, 20, 0)\n                                  ), \n        axis.line.y = element_blank(), \n        axis.title.x = element_text(margin = margin(10, 0, 0, 10)), \n        axis.title.y = element_text(margin = margin(0, 10, 10, 0)), \n        plot.caption = element_text(color = text)\n        ) + \n  labs(color = '', \n       fill = '', \n       x = 'Study size', \n       y = 'Reported effect size', \n       title = 'Publication bias leads to inflated treatment effects', \n       subtitle = 'Most academic journals preferentially publish \"significant\" findings,\\nbiasing towards exaggerated effect sizes, especially in small studies.', \n       caption = '@jessgraves.bsky.social') +\n  annotate(geom='text',\n           label = c('Statistically significant\\n(likely to be published)', \n                     'Not siginficant\\n(unlikely to be published)', \n                     'True effect'),\n          color = c(colors2, 'white'), \n          y=c(3.5, 0.15, 1.2), \n          x=c(15, 35, 5), \n          hjust=0, \n          fontface = 'bold')\n\npal &lt;- \"Mint\"\n# pal &lt;- \"OrRd\"\np1_v2 &lt;- result_data |&gt;\n  filter(n %in% c(4, 8, 12, 20, 60, 100, 200)) |&gt;\n  ggplot(aes(y = factor(n), \n             x= -1*delta, \n             color = n, \n             fill = n)) + \n  geom_density_ridges2(alpha=0.5, \n                       scale=3) + \n  scale_fill_continuous_sequential(palette = pal) + \n  scale_color_continuous_sequential(palette = pal) + \n  scale_x_continuous(expand=c(0, 0), \n                     breaks = scales::pretty_breaks(5), \n                     limits = c(-3, 4)) + \n  scale_y_discrete(expand=c(0, 0)) + \n  labs(x='Observed effect size', \n       y='Study size', \n       subtitle = 'Small studies ‚Üí unreliable estimates'\n       ) + \n  theme(legend.position = 'none', \n        axis.line = element_blank(),\n        axis.text = element_text(size=10), \n        axis.title.x = element_text(#margin = margin(10, 0, 0, 10), \n                                    size = 10), \n        axis.title.y = element_text(#margin = margin(0, 10, 10, 0),\n                                    size = 10, \n                                    hjust=0.2), \n        plot.subtitle = element_text(color = text, \n                                  size = 10,\n                                  hjust = 0.75,\n                                  # margin=margin(5, 0, 20, 0)\n                                  )) + \n  geom_vline(xintercept = 1, \n             color = 'white', \n             linetype = 2, \n             alpha = 0.5) + \n  annotate(geom='text', \n           size=3,\n           label = 'True effect = 1',\n           y=8.5, \n           x=-1.25, \n           # hjust=-1.25, \n           color = 'white', \n           fontface = 'bold')\n\n\n\n\nCode\nfinal_fig &lt;- p2 + \n  inset_element(p1_v2, \n                left = 0.7, \n                right = 0.98, \n                top = 1.2, \n                bottom = 0.5) \n\nfinal_fig\nggsave('preview-image.png', final_fig, \n         units = 'cm', \n         width = 9*3, height = 7*2)\n\n\n\n\n\n\n\n\nFigure¬†3. Publication bias exaggerates treatment effects, particularly when studies are small"
  },
  {
    "objectID": "posts/2025-03-22-small-samples-big-effects/index.html#read-with-caution",
    "href": "posts/2025-03-22-small-samples-big-effects/index.html#read-with-caution",
    "title": "Small samples, big biases",
    "section": "Read with caution",
    "text": "Read with caution\nPublication bias poses substantial problems to science, both when treatment effects are real and not.\n\nWhen there are real effects, we need consistently huge studies to approach unbiased estimates. This is a huge cost to study administrators and patients.\nWhile not presented here, in the case of no real effects, publication bias would encourage acceptance of false positives (those studies that happen to fall into the Type I error).\n\nSo, as we go about reading papers and designing studies, we should think critically about what we are powering, what is being reported, and what it actually means.\nSeems to me the only way to genuinely address this is to conduct many studies on the same topic. Replicate, replicate, replicate. And yet‚Ä¶ often papers are rejected because they are not ‚Äúnovel‚Äù üòµ"
  },
  {
    "objectID": "publications/2022-05-17-ncanda/index.html",
    "href": "publications/2022-05-17-ncanda/index.html",
    "title": "Self-reported sleep and circadian characteristics predict alcohol and cannabis use: A longitudinal analysis of the National Consortium on Alcohol and Neurodevelopment in Adolescence study",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nBACKGROUND:\nGrowing evidence indicates that sleep characteristics predict future substance use and related problems. However, most prior studies assessed a limited range of sleep characteristics, studied a narrow age span, and included few follow-up assessments. Here, we used six annual assessments from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA) study, which spans adolescence and young adulthood with an accelerated longitudinal design, to examine whether multiple sleep characteristics in any year predict alcohol and cannabis use the following year.\nMETHODS:\nThe sample included 831 NCANDA participants (423 females; baseline age 12‚Äì21 years). Sleep variables included circadian preference, sleep quality, daytime sleepiness, timing of midsleep (weekday/weekend), and sleep duration (weekday/weekend). Using generalized linear mixed models (logistic for cannabis; ordinal for binge severity), we tested whether each repeatedly-measured sleep characteristic (years 0‚Äì4) predicted substance use (alcohol binge severity or cannabis use) the following year (years 1‚Äì5), covarying for age, sex, race, visit, parental education, and previous year‚Äôs substance use.\nRESULTS:\nGreater eveningness, more daytime sleepiness, later weekend sleep timing and shorter sleep duration (weekday/weekend) all predicted more severe alcohol bingeing the following year. Only greater eveningness predicted a greater likelihood of any cannabis use the following year. Post-hoc stratified exploratory analyses indicated that some associations (e.g.¬†greater eveningness, shorter weekend sleep duration) predicted binge severity only in female participants, and that middle-high school, versus post-high school, adolescents were more vulnerable to sleep-related risk for cannabis use.\nCONCLUSIONS:\nOur findings support the relevance of multiple sleep/circadian characteristics in the risk for future alcohol binge severity and cannabis use. Preliminary findings suggest that these risk factors vary based on developmental stage and sex. Results underscore a need for greater attention to sleep/circadian characteristics as potential risk factors for substance use in youth, and may inform new avenues to prevention and intervention.\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._hasler2022,\n  author = {A. Hasler, Brant and L. Graves, Jessica and L. Wallace,\n    Meredith and , ...},\n  title = {Self-Reported Sleep and Circadian Characteristics Predict\n    Alcohol and Cannabis Use: {A} Longitudinal Analysis of the\n    {National} {Consortium} on {Alcohol} and {Neurodevelopment} in\n    {Adolescence} Study},\n  date = {2022-05-17},\n  url = {https://www.mdpi.com/1424-8220/21/5/1718},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. Hasler, Brant, Jessica L. Graves, Meredith L. Wallace, and... 2022.\n‚ÄúSelf-Reported Sleep and Circadian Characteristics Predict Alcohol\nand Cannabis Use: A Longitudinal Analysis of the National Consortium on\nAlcohol and Neurodevelopment in Adolescence Study.‚Äù May 17, 2022.\nhttps://www.mdpi.com/1424-8220/21/5/1718."
  },
  {
    "objectID": "publications/2023-02-13-geroscience/index.html",
    "href": "publications/2023-02-13-geroscience/index.html",
    "title": "Evaluating instruments for assessing healthspan: a multi-center cross-sectional study on health-related quality of life (HRQL) and frailty in the companion dog",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nDeveloping valid tools that assess key determinants of canine healthspan such as frailty and health-related quality of life (HRQL) is essential to characterizing and understanding aging in dogs. Additionally, because the companion dog is an excellent translational model for humans, such tools can be applied to evaluate gerotherapeutics and investigate mechanisms underlying longevity in both dogs and humans. In this multi-center, cross-sectional study, we investigated the use of a clinical questionnaire (Canine Frailty Index; CFI; Banzato et al., 2019) to assess frailty and an owner assessment tool (VetMetrica HRQL) to evaluate HRQL in 451 adult companion dogs. Results demonstrated validity of the tools by confirming expectations that frailty score increases and HRQL scores deteriorate with age. CFI scores were significantly higher (higher frailty) and HRQL scores significantly lower (worse HRQL) in old dogs (‚â• 7 years of age) compared to young dogs (‚â• 2 and &lt; 6 years of age). Body size (small &lt; 11.3 kg (25 lbs) or large &gt; 22.7 kg (50 lbs)) was not associated with CFI or total HRQL score. However, older, larger dogs showed faster age-related decline in HRQL scores specific to owner-reported activity and comfort. Findings suggest that the clinician-assessed CFI and owner-reported VetMetrica HRQL are useful tools to evaluate two determinants of healthspan in dogs: the accumulation of frailty and the progressive decline in quality of life. Establishing tools that operationalize the assessment of canine healthspan is critical for the advancement of geroscience and the development of gerotherapeutics that benefit both human and veterinary medicine.\n\n\n\n\n\n\nCitationBibTeX citation:@online{l._chen2023,\n  author = {L. Chen, Frances and V. Ullal, Tarini and L. Graves, Jessica\n    and , ...},\n  title = {Evaluating Instruments for Assessing Healthspan: A\n    Multi-Center Cross-Sectional Study on Health-Related Quality of Life\n    {(HRQL)} and Frailty in the Companion Dog},\n  date = {2023-02-13},\n  url = {https://link.springer.com/article/10.1007/s11357-023-00744-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nL. Chen, Frances, Tarini V. Ullal, Jessica L. Graves, and... 2023.\n‚ÄúEvaluating Instruments for Assessing Healthspan: A Multi-Center\nCross-Sectional Study on Health-Related Quality of Life (HRQL) and\nFrailty in the Companion Dog.‚Äù February 13, 2023. https://link.springer.com/article/10.1007/s11357-023-00744-2."
  },
  {
    "objectID": "publications/2024-06-01-hfd-metab/index.html",
    "href": "publications/2024-06-01-hfd-metab/index.html",
    "title": "Feeding dogs a high-fat diet induces metabolic changes similar to natural aging, including dyslipidemia, hyperinsulinemia, and peripheral insulin resistance",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nOBJECTIVE\nThe goal of this study was to characterize changes induced by a high-fat diet in body composition, insulin levels and sensitivity, blood lipids, and other key biomarkers also associated with the metabolic dysfunction that occurs with natural aging.\nANIMALS\n24 male Beagle dogs, 3 to 7 years of age, of mixed castration status.\nMETHODS\nDogs were randomly assigned to continue twice daily feeding of the commercial adult maintenance diet (n = 12, including 2 intact) that they were previously fed or to a high-fat diet (12, including 2 intact) for 17 weeks between December 1, 2021, and April 28, 2022. Assessments included body composition (weight, body condition score, and adipose mass determined by deuterium enrichment), clinical chemistries, plasma fatty acid quantification, oral glucose tolerance test, and histology of subcutaneous and visceral adipose biopsy samples.\nRESULTS\nThe high-fat diet led to increased body weight, body condition score, fat mass and adipocyte size, hyperinsulinemia and peripheral insulin resistance, and elevations in serum lipids, including cholesterol, triglycerides, and several species of free fatty acids. Leptin levels increased in dogs fed a high-fat diet but not in control dogs. There were no significant changes in routine clinical chemistry values in either group.\nCLINICAL RELEVANCE\nFeeding a high-fat diet for 17 weeks led to potentially deleterious changes in metabolism similar to those seen in natural aging in dogs, including hyperinsulinemia, insulin resistance, and dyslipidemia. A high-fat diet model may provide insights into the similar metabolic dysfunction that occurs during natural aging.\n\n\n\n\n\n\nCitationBibTeX citation:@online{mckenzie2024,\n  author = {McKenzie, Brennen and Peloquin, Matt and Tovar, Ashley and\n    L. Graves, Jessica and , ...},\n  title = {Feeding Dogs a High-Fat Diet Induces Metabolic Changes\n    Similar to Natural Aging, Including Dyslipidemia, Hyperinsulinemia,\n    and Peripheral Insulin Resistance},\n  date = {2024-06-01},\n  url = {https://avmajournals.avma.org/view/journals/ajvr/85/6/ajvr.23.11.0253.xml},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMcKenzie, Brennen, Matt Peloquin, Ashley Tovar, Jessica L. Graves,\nand... 2024. ‚ÄúFeeding Dogs a High-Fat Diet Induces Metabolic\nChanges Similar to Natural Aging, Including Dyslipidemia,\nHyperinsulinemia, and Peripheral Insulin Resistance.‚Äù June 1,\n2024. https://avmajournals.avma.org/view/journals/ajvr/85/6/ajvr.23.11.0253.xml."
  },
  {
    "objectID": "publications/2021-03-02-sensors/index.html",
    "href": "publications/2021-03-02-sensors/index.html",
    "title": "Profiles of Accelerometry-Derived Physical Activity Are Related to Perceived Physical Fatigability in Older Adults",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nPhysical activity (PA) is associated with greater fatigability in older adults; little is known about magnitude, shape, timing and variability of the entire 24-h rest‚Äìactivity rhythm (RAR) associated with fatigability. We identified which features of the 24-h RAR pattern were independently and jointly associated with greater perceived physical fatigability (Pittsburgh Fatigability Scale, PFS, 0‚Äì50) in older adults (n¬†= 181, 71.3 ¬± 6.7 years). RARs were characterized using anti-logistic extended cosine models and 4-h intervals of PA means and standard deviations across days. A K-means clustering algorithm approach identified four profiles of RAR features: ‚ÄúLess Active/Robust‚Äù, ‚ÄúEarlier Risers‚Äù, ‚ÄúMore Active/Robust‚Äù and ‚ÄúLater RAR‚Äù. Quantile regression tested associations of each RAR feature/profile on median PFS adjusted for age, sex, race, body mass index and depression symptomatology. Later rise times (up mesor; Œ≤ = 1.38,¬†p¬†= 0.01) and timing of midpoint of activity (acrophase; Œ≤ = 1.29,¬†p¬†= 0.01) were associated with higher PFS scores. Lower PA between 4 a.m. and 8 a.m. was associated with higher PFS scores (Œ≤ = ‚àí4.50,¬†p¬†= 0.03). ‚ÄúLess Active/Robust‚Äù (Œ≤ = 6.14,¬†p¬†= 0.01) and ‚ÄúLater RAR‚Äù (Œ≤ = 3.53,¬†p¬†= 0.01) patterns were associated with higher PFS scores compared to ‚ÄúEarlier Risers‚Äù. Greater physical fatigability in older adults was associated with dampened, more variable, and later RARs. This work can guide development of interventions aimed at modifying RARs to reduce fatigability in older adults.\n\n\n\n\n\n\nCitationBibTeX citation:@online{l._graves2021,\n  author = {L. Graves, Jessica and (Susanna) Qiao, Yujia and D. Moored,\n    Kyle and , ...},\n  title = {Profiles of {Accelerometry-Derived} {Physical} {Activity}\n    {Are} {Related} to {Perceived} {Physical} {Fatigability} in {Older}\n    {Adults}},\n  date = {2021-03-02},\n  url = {https://www.mdpi.com/1424-8220/21/5/1718},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nL. Graves, Jessica, Yujia (Susanna) Qiao, Kyle D. Moored, and... 2021.\n‚ÄúProfiles of Accelerometry-Derived Physical Activity Are Related\nto Perceived Physical Fatigability in Older Adults.‚Äù March 2,\n2021. https://www.mdpi.com/1424-8220/21/5/1718."
  },
  {
    "objectID": "publications/2025-02-13_healthspan-biomarkers/index.html",
    "href": "publications/2025-02-13_healthspan-biomarkers/index.html",
    "title": "Changes in insulin, adiponectin and lipid concentrations with age are associated with frailty and reduced quality of life in dogs",
    "section": "",
    "text": "Read full article here.\n\nAbstract\nDeclining metabolic function with aging is a conserved phenotype across many species. While aging-associated changes in metabolic status have been investigated rigorously in humans, less is known about metabolic aging in dogs. In this cross-sectional study, we aimed to examine changes in metabolic health with age, and any associations with frailty and quality of life, in a diverse population of companion dogs. This cross-sectional study enrolled 451 mature, adult companion dogs. Serum adiponectin, ALP, ALT, AST, cholesterol, insulin, IGF-1 and glucose levels were quantified. Additionally, plasma FFA, SFA, PA, OA and LA were quantified in a 61 dog subpopulation. All analytes were significantly associated with age, with the exception of AST. Elevated ALP, ALT, cholesterol, insulin, FFA, PA and OA were correlated with increased frailty scores, while higher levels of glucose and adiponectin were correlated with reduced frailty scores. The strength of these associations increased with age. Higher ALP, ALT and insulin were associated with lower HRQL scores after adjusting for covariates. Our findings establish novel associations between deleterious aging-associated metabolic changes and validated measures of clinical well-being in companion dogs. Future research should investigate the causality of these associations to inform therapeutic strategies targeting age-associated changes to frailty and quality of life.\n\n\n\n\n\n\nCitationBibTeX citation:@online{mckenzie2025,\n  author = {McKenzie, Brennen and Peloquin, Matthew and L. Graves,\n    Jessica and , ...},\n  title = {Changes in Insulin, Adiponectin and Lipid Concentrations with\n    Age Are Associated with Frailty and Reduced Quality of Life in Dogs},\n  date = {2025-02-13},\n  url = {https://www.nature.com/articles/s41598-025-89923-z},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMcKenzie, Brennen, Matthew Peloquin, Jessica L. Graves, and... 2025.\n‚ÄúChanges in Insulin, Adiponectin and Lipid Concentrations with Age\nAre Associated with Frailty and Reduced Quality of Life in Dogs.‚Äù\nFebruary 13, 2025. https://www.nature.com/articles/s41598-025-89923-z."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "üßÆ Statistics & data science\nüíä Clinical trials & aging & drug development\nüíª R enthusiast\nüë©‚Äçüíª Statistician @ loyal.com\n\nI‚Äôm a (bio)statistician/data scientist(/whatever you want to call it) working in aging, clinical trials and drug development.\nI have been doing applied statistics for almost a decade now (yeesh time flies), covering a variety of areas like, epidemiology, occupational health, circadian rhythms, actigraphy, depression & substance use.\nRight now, my statistical interests include: everything Rstats, longitudinal data analysis, simulation-based power analysis, and survival analysis.\nI‚Äôm also a mom of twins üëØ‚Äç‚ôÇÔ∏è who loves horror movies üé• and is currently obsessed with Skyrim üßô."
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "About",
    "section": "",
    "text": "üßÆ Statistics & data science\nüíä Clinical trials & aging & drug development\nüíª R enthusiast\nüë©‚Äçüíª Statistician @ loyal.com\n\nI‚Äôm a (bio)statistician/data scientist(/whatever you want to call it) working in aging, clinical trials and drug development.\nI have been doing applied statistics for almost a decade now (yeesh time flies), covering a variety of areas like, epidemiology, occupational health, circadian rhythms, actigraphy, depression & substance use.\nRight now, my statistical interests include: everything Rstats, longitudinal data analysis, simulation-based power analysis, and survival analysis.\nI‚Äôm also a mom of twins üëØ‚Äç‚ôÇÔ∏è who loves horror movies üé• and is currently obsessed with Skyrim üßô."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\nStatistical programming: R (expert), Python (proficient), SAS (trained in, but rarely use)\nStatistical methods: Longitudinal data analysis, study design & power, survival analysis, unsupervised learning\nProfessional: Regulatory & academic report writing, cross-functional teams, stats communication"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nI have an MS in both Biostatistics (2018) and Epidemiology (2020) from the University of Pittsburgh, Graduate School of Public Health."
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\nStatistics Manager, Research\nLoyal | 2021-Present\n(Data Scientist \\(\\to\\) Senior Data Scientist \\(\\to\\) Senior Statistician \\(\\to\\) Statistics Manager)\nStatistician \nUniversity of Pittsburgh Medical Center | 2020-2021\nGraduate Student & Researcher in Biostatistics & Epidemiology\nGraduate School of Public Health, University of Pittsburgh | 2016-2020\nStaff Biostatistician\nCenter for Occupational Biostatistics and Epidemiology | 2017-2018"
  },
  {
    "objectID": "index.html#helpful-resources",
    "href": "index.html#helpful-resources",
    "title": "About",
    "section": "Helpful resources",
    "text": "Helpful resources\n\nCsik, Samantha. 2022.¬†‚ÄúAdding a Blog to Your Existing Quarto Website.‚Äù¬†\nRapp, Albert. 2022. ‚ÄúThe ultimate guide to starting a Quarto blog.‚Äù"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n[1] 2"
  }
]